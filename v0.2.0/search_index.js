var documenterSearchIndex = {"docs":
[{"location":"api/#API-1","page":"API","title":"API","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"DocTestSetup  = quote\n    using MGVI\nend","category":"page"},{"location":"api/#Components-listing-1","page":"API","title":"Components listing","text":"","category":"section"},{"location":"api/#Modules-1","page":"API","title":"Modules","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"Order = [:module]","category":"page"},{"location":"api/#Types-and-constants-1","page":"API","title":"Types and constants","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"Order = [:type, :constant]","category":"page"},{"location":"api/#Functions-and-macros-1","page":"API","title":"Functions and macros","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"Order = [:macro, :function]","category":"page"},{"location":"api/#Documentation-1","page":"API","title":"Documentation","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"Modules = [MGVI]\nOrder = [:module, :type, :constant, :macro, :function]","category":"page"},{"location":"api/#MGVI.MGVI","page":"API","title":"MGVI.MGVI","text":"MGVI\n\nAn implementation of the Metric Gaussian Variational Inference algorithm.\n\n\n\n\n\n","category":"module"},{"location":"api/#MGVI.FullJacobianFunc","page":"API","title":"MGVI.FullJacobianFunc","text":"FullJacobianFunc(f)\n\nConstruct of the Jacobian with ForwardDiff.\n\nWhen called at point θ, the Jacobian matrix is fully instantiated and stored explicitly in memory.\n\nExamples\n\n# forward_model: θ -> Distribution\njacobian_func = FullJacobianFunc(forward_model)\njacobian = jacobian_func(θ)  # LinearMap\njacobian * v  # act on vector\n\n\n\n\n\n","category":"type"},{"location":"api/#MGVI.FullResidualSampler","page":"API","title":"MGVI.FullResidualSampler","text":"FullResidualSampler(λ_information, jac_dλ_dθ)\n\nGaussian posterior's covariance approximated by the Fisher Information\n\nCaution: This sampler constructs the covariance matrix explicitly, so memory use grows quadratically with the number of model parameters\n\nThe Fisher information in canonical coordinates and Jacobian of the coordinate transformation are provided as arguments. Since J cdot I_canon cdot J^T does not depend on the definition of canonical coordinates, we omit description of what canonical coordinates are.\n\nArguments\n\nλ_information::LinearMap: Fisher Information in canonical coordinates. Coordinates are chosen in the way that this matrix is very simple, and diagonal for the univariate distributions\njac_dλ_fθ::LinearMap: Coordinate transformation jacobian between canonical coordinates and coordinates of the model\n\n\n\n\n\n","category":"type"},{"location":"api/#MGVI.FwdDerJacobianFunc","page":"API","title":"MGVI.FwdDerJacobianFunc","text":"FwdDerJacobianFunc(f)\n\nConstruct of the Jacobian with ForwardDiff for the direct action, and twice applied ForwardDiff for the adjoint action\n\nAdjoint is implemented by introducing a placeholder parametric vector vect:\n\nfracddvect vecx cdot (A vect) = A^T vecx\n\nThis allows to implement both, the direct and adjoint actions, using ForwardDiff, without instantiating full Jacobian at any point.\n\n\n\n\n\n","category":"type"},{"location":"api/#MGVI.FwdRevADJacobianFunc","page":"API","title":"MGVI.FwdRevADJacobianFunc","text":"FwdRevADJacobianFunc(f)\n\nConstruct of the Jacobian with ForwardDiff for direct action and Zygote.pullback for the adjoint\n\nThe Jacobian action is computed on the fly, no matrix is stored in memory at any time.\n\nExamples\n\n# forward_model: θ -> Distribution\njacobian_func = FwdRevADJacobianFunc(forward_model)\njacobian = jacobian_func(θ)  # LinearMap\njacobian * v  # act on vector as if it was a matrix\n\n\n\n\n\n","category":"type"},{"location":"api/#MGVI.ImplicitResidualSampler","page":"API","title":"MGVI.ImplicitResidualSampler","text":"ImplicitResidualSampler(λ_information, jac_dλ_dθ; cg_params=NamedTuple())\n\nMemory efficient Gaussian posterior's covariance approximated by the Fisher Information\n\nThis sampler uses Conjugate Gradients to iteratively invert  the Fisher information, never instantiating the covariance in memory explicitly.\n\nArguments\n\nλ_information::LinearMap: Fisher Information in canonical coordinates.\njac_dλ_fθ::LinearMap: Coordinate transformation jacobian between canonical coordinates and\ncg_params::NamedTuple: Keyword arguments passed to cg. Useful for enabling verbose mode or to set accuracy/number of iterations\n\n\n\n\n\n","category":"type"},{"location":"api/#MGVI.mgvi_kl_optimize_step-Union{Tuple{RS}, Tuple{JF}, Tuple{Random.AbstractRNG,Function,Any,AbstractArray{T,1} where T}} where RS<:MGVI.AbstractResidualSampler where JF<:MGVI.AbstractJacobianFunc","page":"API","title":"MGVI.mgvi_kl_optimize_step","text":"mgvi_kl_optimize_step(rng, forward_model, data, init_param_point;\n                      jacobian_func=jacobian_func,\n                      residual_sampler=residual_sampler,\n                      [num_residuals=3,]\n                      [residual_sampler_options=NamedTuple(),]\n                      [optim_solver=LBFGS(),]\n                      [optim_options=Optim.Options()])\n\nPerforms one MGVI iteration.\n\nThe posterior distribution is approximated with a multivariate normal distribution. The covariance is approximated with the inverse Fisher information valuated at init_param_point. Samples are drawn according to this covariance, which are then used to estimate and minimize the KL divergence between the true posterior and the approximation.\n\nArguments\n\nrng::AbstractRNG: instance of the random number generator\nforward_model::Function: turns model parameters into an instance of Distribution\ndata::AbstractVector: data on which model's pdf is evaluated\ninit_param_point::Vector: initial estimate of the model parameters\njacobian_func::Type{<:AbstractJacobianFunc}: method to calculate the Jacobian of the forward model\nresidual_sampler::Type{<:AbstractResidualSampler}: method to draw samples from the approximation\nnum_residuals::Integer = 3: number of samples used to estimate the KL divergence\nresidual_sampler_options::NamedTuple = NamedTuple(): further options to pass to the residual sampler. Important is cg_params that is passed to the CG solver used inside of ImplicitResidualSampler\noptim_solver::Optim.AbstractOptimizer = LBFGS(): optimizer used for minimizing KL divergence\noptim_options::Optim.Options = Optim.Options(): options to pass to the KL optimizer\n\nExample\n\nusing Random, Distributions, MGVI\n\nmodel(x::AbstractVector) = Normal(x[1], 0.2)\ntrue_param = [2.0]\ndata = rand(model(true_param), 1)[1]\ninit_param = [1.3]\n\nres = mgvi_kl_optimize_step(Random.GlobalRNG, model, data, init_param;\n                            jacobian_func=FwdRevADJacobianFunc,\n                            residual_sampler=ImplicitResidualSampler,\n                            num_residuals=5,\n                            residual_sampler_options=(;cg_params=(;maxiter=10, verbose=true))),\n                            optim_solver=LBFGS(;m=5),\n                            optim_options=Optim.Options(iterations=7, show_trace=true))\n\nnext_param_point = res.result\n\noptim_optimized_object = res.optimized\nOptim.summary(optim_optimized_object)\n\nsamples_from_est_covariance = res.samples\n\nSee also\n\nResidual samplers: AbstractResidualSampler, ImplicitResidualSampler, FullResidualSampler\nJacobian functions: AbstractJacobianFunc, FwdRevADJacobianFunc, FwdDerJacobianFunc\n\n\n\n\n\n","category":"method"},{"location":"api/#MGVI.AbstractJacobianFunc","page":"API","title":"MGVI.AbstractJacobianFunc","text":"abstract type AbstractJacobianFunc <: Function end\n\nAbstract type of the jacobian calculators.\n\nRepresents the jacobian of the function. The instance can be called at the point θ, a LinearMap representing the jacobian is returned.\n\n\n\n\n\n","category":"type"},{"location":"api/#MGVI.AbstractResidualSampler","page":"API","title":"MGVI.AbstractResidualSampler","text":"abstract type AbstractResidualSampler <: Sampleable{Multivariate, Continuous} end\n\nGenerate zero-mean samples from Gaussian posterior, assuming priors are standard gaussians.\n\nExample\n\nrng = MersenneTwister(145)\nrs = FullResidualSampler(λ_information, jac_dλ_dθ)\nsamples = rand(rng, rs, 10)  # generate 10 samples\n\n\n\n\n\n","category":"type"},{"location":"LICENSE/#LICENSE-1","page":"LICENSE","title":"LICENSE","text":"","category":"section"},{"location":"LICENSE/#","page":"LICENSE","title":"LICENSE","text":"using Markdown\nMarkdown.parse_file(joinpath(@__DIR__, \"..\", \"..\", \"..\", \"LICENSE.md\"))","category":"page"},{"location":"#MGVI.jl-1","page":"Home","title":"MGVI.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"MGVI is an iterative method that performs a series of Gaussian approximations to the posterior. We alternate between approximating the covariance with the inverse Fisher information metric evaluated at an intermediate mean estimate and optimizing the KL-divergence for the given covariance with respect to the mean. This procedure is iterated until the uncertainty estimate is self-consistent with the mean parameter. We achieve linear scaling by avoiding to store the covariance explicitly at any time. Instead we draw samples from the approximating distribution relying on an implicit representation and numerical schemes to approximately solve linear equations. Those samples are used to approximate the KL-divergence and its gradient. The usage of natural gradient descent allows for rapid convergence. Formulating the Bayesian model in standardized coordinates makes MGVI applicable to any inference problem with continuous parameters. ","category":"page"},{"location":"#Citing-MGVI.jl-1","page":"Home","title":"Citing MGVI.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"When using MGVI.jl for research, teaching or similar, please cite this publication: Metric Gaussian Variational Inference","category":"page"},{"location":"#","page":"Home","title":"Home","text":"@article{knollmüller2020metric,\n         title={Metric Gaussian Variational Inference},\n         author={Jakob Knollmüller and Torsten A. Enßlin},\n         year={2020},\n         eprint={1901.11033},\n         archivePrefix={arXiv},\n         primaryClass={stat.ML}\n}","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"EditURL = \"https://github.com/bat/MGVI.jl/blob/master/docs/build/src/tutorial_lit.jl\"","category":"page"},{"location":"tutorial/#Tutorial-1","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Notebook download nbviewer source","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using MGVI","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using Distributions\nusing Random\nusing ValueShapes\nusing LinearAlgebra\nusing Optim","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"We want to fit a 3-degree polynomial using two data sets (a and b). MGVI requires a model expressed as a function of the model parameters and returning an instance of the Distribution. In this example, since we have two sets of independent measurements, we express them as ValueShapes.NamedTupleDist.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"We assume errors are normally distributed with unknown covariance, which has to be learned as well.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"const _x1_grid = [Float64(i)/10 for i in 1:25]\nconst _x2_grid = [Float64(i)/10 + 0.1 for i in 1:15]\nconst _common_grid = sort(vcat(_x1_grid, _x2_grid))\n\nfunction _mean(x_grid, p)\n    p[1]*10 .+ p[2]*40 .* x_grid .+ p[3]*600 .* x_grid.^2 .+ p[4]*80 .* x_grid.^3\nend\n\nfunction model(p)\n    dist1 = Product(Normal.(_mean(_x1_grid, p), p[5]^2*60))\n    dist2 = Product(Normal.(_mean(_x2_grid, p), p[5]^2*60))\n    NamedTupleDist(a=dist1,\n                   b=dist2)\nend","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"model (generic function with 1 method)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Here we define the ground truth of the parameters, as well as an initial guess.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"const true_params =  [\n -0.3\n -1.5\n 0.2\n -0.5\n 0.3]\n\nconst starting_point = [\n  0.2\n  0.5\n  -0.1\n  0.3\n -0.6\n];\nnothing #hide","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"function pprintln(obj)\n    show(stdout, \"text/plain\", obj)\n    println()\nend;\nnothing #hide","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using Plots\ngr(size=(400, 300), dpi=700, fmt=:png)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Plots.GRBackend()","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"rng = MersenneTwister(157);\nnothing #hide","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"We draw data directly from the model, using the true parameter values:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"data = rand(rng, model(true_params), 1)[1];\nnothing #hide","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"function _mean(x::Vector)\n    _mean(_common_grid, x)\nend\n\ninit_plots =() -> let\n    truth = _mean(true_params)\n    plot!(_common_grid, truth, markercolor=:blue, linecolor=:blue, label=\"truth\")\n    scatter!(_common_grid, _mean(starting_point), markercolor=:orange, markerstrokewidth=0, markersize=3, label=\"init\")\n    scatter!(vcat(_x1_grid, _x2_grid), reduce(vcat, data), markercolor=:black, markerstrokewidth=0, markersize=3, label=\"data\")\nend;\nnothing #hide","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Before we start the optimization, let's have an initial look at the data. It is also interesting to see how our starting guess performs.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"p = plot()\ninit_plots()","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Now we are ready to run one iteration of the MGVI. The output contains an updated parameter estimate (first_iteration.result), which we can compare to the true parameters.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"first_iteration = mgvi_kl_optimize_step(rng,\n                                        model, data,\n                                        starting_point;\n                                        jacobian_func=FwdRevADJacobianFunc,\n                                        residual_sampler=ImplicitResidualSampler,\n                                        optim_options=Optim.Options(iterations=10, show_trace=true),\n                                        residual_sampler_options=(;cg_params=(;maxiter=10)))\npprintln(hcat(first_iteration.result, true_params))\np","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"plot_iteration = (params, label) -> let\n    #error_mat = mgvi_kl_errors(full_model, params)\n    #display(error_mat)\n    #errors = sqrt.(error_mat[diagind(error_mat)])\n    #yerr = abs.(line(common_grid, params+errors) - line(common_grid, params-errors))\n    #scatter!(common_grid, line(common_grid, params), markercolor=:green, label=label, yerr=yerr)\n    for sample in eachcol(params.samples)\n        scatter!(_common_grid, _mean(Vector(sample)), markercolor=:gray, markeralpha=0.3, markersize=2, label=nothing)\n    end\n    scatter!(_common_grid, _mean(params.result), markercolor=:green, label=label)\nend;\nnothing #hide","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Now let's also plot the curve corresponding to the new parameters after the first iteration:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"p = plot()\ninit_plots()\nplot_iteration(first_iteration, \"first\")\np","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"plot_iteration_light = (params, counter) -> let\n    scatter!(_common_grid, _mean(params.result), markercolor=:green, markersize=3, markeralpha=2*atan(counter/18)/π, label=nothing)\nend;\nnothing #hide","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"From the plot above we see that one iteration is not enough. Let's do 5 more steps and plot the evolution of estimates.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"init_plots()\nplt = scatter()\nnext_iteration = first_iteration\nfor i in 1:5\n    pprintln(minimum(next_iteration.optimized))\n    pprintln(hcat(next_iteration.result, true_params))\n    global next_iteration = mgvi_kl_optimize_step(rng,\n                                                  model, data,\n                                                  next_iteration.result;\n                                                  jacobian_func=FwdRevADJacobianFunc,\n                                                  residual_sampler=ImplicitResidualSampler,\n                                                  optim_options=Optim.Options(iterations=10, show_trace=true),\n                                                  residual_sampler_options=(;cg_params=(;maxiter=10)))\n    plot_iteration_light(next_iteration, i)\nend\npprintln(minimum(next_iteration.optimized))\npprintln(hcat(next_iteration.result, true_params))\nplt","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Finally, let's plot the last estimate and compare it to the truth. Also, notice, that gray dots represent samples from the approximation.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"p = plot()\ninit_plots()\nplot_iteration(next_iteration, \"last\")\np","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"This page was generated using Literate.jl.","category":"page"}]
}
