var documenterSearchIndex = {"docs":
[{"location":"api/#API-1","page":"API","title":"API","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"DocTestSetup  = quote\n    using MGVI\nend","category":"page"},{"location":"api/#Components-listing-1","page":"API","title":"Components listing","text":"","category":"section"},{"location":"api/#Modules-1","page":"API","title":"Modules","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"Order = [:module]","category":"page"},{"location":"api/#Types-and-constants-1","page":"API","title":"Types and constants","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"Order = [:type, :constant]","category":"page"},{"location":"api/#Functions-and-macros-1","page":"API","title":"Functions and macros","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"Order = [:macro, :function]","category":"page"},{"location":"api/#Documentation-1","page":"API","title":"Documentation","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"Modules = [MGVI]\nOrder = [:module, :type, :constant, :macro, :function]","category":"page"},{"location":"api/#MGVI.MGVI","page":"API","title":"MGVI.MGVI","text":"MGVI\n\nAn implementation of the Metric Gaussian Variational Inference algorithm.\n\n\n\n\n\n","category":"module"},{"location":"api/#MGVI.FullJacobianFunc","page":"API","title":"MGVI.FullJacobianFunc","text":"FullJacobianFunc(f)\n\nConstruct of the Jacobian with ForwardDiff.\n\nWhen called at point θ, the Jacobian matrix is fully instantiated and stored explicitly in memory.\n\nExamples\n\n# forward_model: θ -> Distribution\njacobian_func = FullJacobianFunc(forward_model)\njacobian = jacobian_func(θ)  # LinearMap\njacobian * v  # act on vector\n\n\n\n\n\n","category":"type"},{"location":"api/#MGVI.FullResidualSampler","page":"API","title":"MGVI.FullResidualSampler","text":"FullResidualSampler(λ_information, jac_dλ_dθ)\n\nGaussian posterior's covariance approximated by the Fisher Information\n\nCaution: This sampler constructs the covariance matrix explicitly, so memory use grows quadratically with the number of model parameters\n\nThe Fisher information in canonical coordinates and Jacobian of the coordinate transformation are provided as arguments. Since J cdot I_canon cdot J^T does not depend on the definition of canonical coordinates, we omit description of what canonical coordinates are.\n\nArguments\n\nλ_information::LinearMap: Fisher Information in canonical coordinates. Coordinates are chosen in the way that this matrix is very simple, and diagonal for the univariate distributions\njac_dλ_fθ::LinearMap: Coordinate transformation jacobian between canonical coordinates and coordinates of the model\n\n\n\n\n\n","category":"type"},{"location":"api/#MGVI.FwdDerJacobianFunc","page":"API","title":"MGVI.FwdDerJacobianFunc","text":"FwdDerJacobianFunc(f)\n\nConstruct of the Jacobian with ForwardDiff for the direct action, and twice applied ForwardDiff for the adjoint action\n\nAdjoint is implemented by introducing a placeholder parametric vector vect:\n\nfracddvect vecx cdot (A vect) = A^T vecx\n\nThis allows to implement both, the direct and adjoint actions, using ForwardDiff, without instantiating full Jacobian at any point.\n\n\n\n\n\n","category":"type"},{"location":"api/#MGVI.FwdRevADJacobianFunc","page":"API","title":"MGVI.FwdRevADJacobianFunc","text":"FwdRevADJacobianFunc(f)\n\nConstruct of the Jacobian with ForwardDiff for direct action and Zygote.pullback for the adjoint\n\nThe Jacobian action is computed on the fly, no matrix is stored in memory at any time.\n\nExamples\n\n# forward_model: θ -> Distribution\njacobian_func = FwdRevADJacobianFunc(forward_model)\njacobian = jacobian_func(θ)  # LinearMap\njacobian * v  # act on vector as if it was a matrix\n\n\n\n\n\n","category":"type"},{"location":"api/#MGVI.ImplicitResidualSampler","page":"API","title":"MGVI.ImplicitResidualSampler","text":"ImplicitResidualSampler(λ_information, jac_dλ_dθ; cg_params=NamedTuple())\n\nMemory efficient Gaussian posterior's covariance approximated by the Fisher Information\n\nThis sampler uses Conjugate Gradients to iteratively invert  the Fisher information, never instantiating the covariance in memory explicitly.\n\nArguments\n\nλ_information::LinearMap: Fisher Information in canonical coordinates.\njac_dλ_fθ::LinearMap: Coordinate transformation jacobian between canonical coordinates and\ncg_params::NamedTuple: Keyword arguments passed to cg. Useful for enabling verbose mode or to set accuracy/number of iterations\n\n\n\n\n\n","category":"type"},{"location":"api/#MGVI.mgvi_kl_optimize_step-Union{Tuple{RS}, Tuple{JF}, Tuple{Random.AbstractRNG, Function, Any, AbstractVector{T} where T}} where {JF<:MGVI.AbstractJacobianFunc, RS<:MGVI.AbstractResidualSampler}","page":"API","title":"MGVI.mgvi_kl_optimize_step","text":"mgvi_kl_optimize_step(rng, forward_model, data, init_param_point;\n                      jacobian_func=jacobian_func,\n                      residual_sampler=residual_sampler,\n                      [num_residuals=3,]\n                      [residual_sampler_options=NamedTuple(),]\n                      [optim_solver=LBFGS(),]\n                      [optim_options=Optim.Options()])\n\nPerforms one MGVI iteration.\n\nThe posterior distribution is approximated with a multivariate normal distribution. The covariance is approximated with the inverse Fisher information valuated at init_param_point. Samples are drawn according to this covariance, which are then used to estimate and minimize the KL divergence between the true posterior and the approximation.\n\nArguments\n\nrng::AbstractRNG: instance of the random number generator\nforward_model::Function: turns model parameters into an instance of Distribution\ndata::AbstractVector: data on which model's pdf is evaluated\ninit_param_point::Vector: initial estimate of the model parameters\njacobian_func::Type{<:AbstractJacobianFunc}: method to calculate the Jacobian of the forward model\nresidual_sampler::Type{<:AbstractResidualSampler}: method to draw samples from the approximation\nnum_residuals::Integer = 3: number of samples used to estimate the KL divergence\nresidual_sampler_options::NamedTuple = NamedTuple(): further options to pass to the residual sampler. Important is cg_params that is passed to the CG solver used inside of ImplicitResidualSampler\noptim_solver::Optim.AbstractOptimizer = LBFGS(): optimizer used for minimizing KL divergence\noptim_options::Optim.Options = Optim.Options(): options to pass to the KL optimizer\n\nExample\n\nusing Random, Distributions, MGVI\n\nmodel(x::AbstractVector) = Normal(x[1], 0.2)\ntrue_param = [2.0]\ndata = rand(model(true_param), 1)[1]\ninit_param = [1.3]\n\nres = mgvi_kl_optimize_step(Random.GlobalRNG, model, data, init_param;\n                            jacobian_func=FwdRevADJacobianFunc,\n                            residual_sampler=ImplicitResidualSampler,\n                            num_residuals=5,\n                            residual_sampler_options=(;cg_params=(;maxiter=10, verbose=true))),\n                            optim_solver=LBFGS(;m=5),\n                            optim_options=Optim.Options(iterations=7, show_trace=true))\n\nnext_param_point = res.result\n\noptim_optimized_object = res.optimized\nOptim.summary(optim_optimized_object)\n\nsamples_from_est_covariance = res.samples\n\nSee also\n\nResidual samplers: AbstractResidualSampler, ImplicitResidualSampler, FullResidualSampler\nJacobian functions: AbstractJacobianFunc, FwdRevADJacobianFunc, FwdDerJacobianFunc\n\n\n\n\n\n","category":"method"},{"location":"api/#MGVI.AbstractJacobianFunc","page":"API","title":"MGVI.AbstractJacobianFunc","text":"abstract type AbstractJacobianFunc <: Function end\n\nAbstract type of the jacobian calculators.\n\nRepresents the jacobian of the function. The instance can be called at the point θ, a LinearMap representing the jacobian is returned.\n\n\n\n\n\n","category":"type"},{"location":"api/#MGVI.AbstractResidualSampler","page":"API","title":"MGVI.AbstractResidualSampler","text":"abstract type AbstractResidualSampler <: Sampleable{Multivariate, Continuous} end\n\nGenerate zero-mean samples from Gaussian posterior, assuming priors are standard gaussians.\n\nExample\n\nrng = MersenneTwister(145)\nrs = FullResidualSampler(λ_information, jac_dλ_dθ)\nsamples = rand(rng, rs, 10)  # generate 10 samples\n\n\n\n\n\n","category":"type"},{"location":"LICENSE/#LICENSE-1","page":"LICENSE","title":"LICENSE","text":"","category":"section"},{"location":"LICENSE/#","page":"LICENSE","title":"LICENSE","text":"using Markdown\nMarkdown.parse_file(joinpath(@__DIR__, \"..\", \"..\", \"..\", \"LICENSE.md\"))","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"EditURL = \"https://github.com/bat/MGVI.jl/blob/master/docs/build/src/advanced_tutorial_lit.jl\"","category":"page"},{"location":"advanced_tutorial/#Advanced-Tutorial-1","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"","category":"section"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Notebook download nbviewer source","category":"page"},{"location":"advanced_tutorial/#Introduction-1","page":"Advanced Tutorial","title":"Introduction","text":"","category":"section"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this tutorial, we will fit the coal mining disaster dataset with a Gaussian process modulated Poisson process.","category":"page"},{"location":"advanced_tutorial/#Prepare-the-environment-1","page":"Advanced Tutorial","title":"Prepare the environment","text":"","category":"section"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We start by importing:","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"MGVI for the posterior fit\nDistributions.jl and FFTW.jl to define the statistical model\nOptim.jl to pass Optim.Options to MGVI and to find Maximum a posteriori fit that we will use for comparison\nStatsBase.jl for histogram construction from the data and also for error bands visualization\nPlots.jl for visualization","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"using MGVI\n\nusing Distributions\nusing DelimitedFiles\nusing Random\nusing Optim\nusing StatsBase\n\nusing Plots\nusing Plots.PlotMeasures\nPlots.default(legendfontsize=10, tickfontsize=10, grid=false, dpi=120, size=(500, 300))\n\nusing FFTW\n\nimport ForwardDiff","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Random.seed!(84612);\nnothing #hide","category":"page"},{"location":"advanced_tutorial/#Load-data-1","page":"Advanced Tutorial","title":"Load data","text":"","category":"section"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The dataset, which is included with this repository, contains intervals in days between disasters occuring at British coal mines between March 1851 and March 1962. We build a model by splitting the entire time range into intervals of 365 days.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"function read_coal_mining_data(filepath, binsize)\n    init_year = empty\n    data = empty\n    open(filepath) do io\n        raw = readline(io)\n        while ! occursin(\"init_date\", raw)\n            raw = readline(io)\n        end\n\n        init_year = parse(Float64, split(split(strip(raw[2:end]), \"\\t\")[2], \"-\")[1])\n        data = readdlm(io, '\\t', Int, '\\n', comments=true)[:]\n    end\n    dates_fract_years = init_year .+ cumsum(data)/365\n    left_edge = dates_fract_years[1]\n    num_bins = ((dates_fract_years[end] - left_edge) ÷ binsize)\n    right_edge = left_edge + binsize*num_bins\n    fit(Histogram, dates_fract_years, left_edge:binsize:right_edge).weights\nend\n\ncoal_mine_disaster_data = read_coal_mining_data(joinpath(\"/home/runner/work/MGVI.jl/MGVI.jl/docs/build/src\", \"coal_mining_data.tsv\"), 1);\nnothing #hide","category":"page"},{"location":"advanced_tutorial/#Global-parameters-and-the-grid-1","page":"Advanced Tutorial","title":"Global parameters and the grid","text":"","category":"section"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Now we define several model properties:","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"DATA_DIM is the shape of the dataset\nDATA_XLIM specifies the time range of the data\nGP_GRAIN_FACTOR determines the numbers of finer bins which a data bin is split into.  This is useful when there are several datasets defined on different grids.\nGP_PADDING adds empty paddings to the dataset. We use a Fourier transform to sample from the Gaussian process  with a finite correlation length. GP_PADDING helps us to ensure that periodic boundary conditions  imposed by a Fourier transform won't affect the data region.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"DATA_DIM = size(coal_mine_disaster_data, 1);\n\ndata = coal_mine_disaster_data;\n\nDATA_XLIM = [1851., 1962.];\n\nGP_GRAIN_FACTOR = 3;\nGP_PADDING = 80;\nnothing #hide","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"function produce_bins()\n    data_binsize = (DATA_XLIM[2] - DATA_XLIM[1])/DATA_DIM\n    gp_binsize = data_binsize/GP_GRAIN_FACTOR\n    gp_dim = Integer(((DATA_XLIM[2] - DATA_XLIM[1]) + 2*GP_PADDING) ÷ gp_binsize)\n    gp_left_bin_offset = gp_right_bin_offset = (gp_dim - DATA_DIM) ÷ 2\n    if (2*gp_left_bin_offset + DATA_DIM*GP_GRAIN_FACTOR) % 2 == 1\n        gp_left_bin_offset += 1\n    end\n    gp_left_xlim = DATA_XLIM[1] - gp_left_bin_offset*gp_binsize\n    gp_right_xlim = DATA_XLIM[2] + gp_right_bin_offset*gp_binsize\n    gp_left_xs = collect(gp_left_xlim + gp_binsize/2:gp_binsize:DATA_XLIM[1])\n    gp_right_xs = collect(DATA_XLIM[2] + gp_binsize/2:gp_binsize:gp_right_xlim)\n    gp_data_xs = collect(DATA_XLIM[1] + gp_binsize/2:gp_binsize:DATA_XLIM[2])\n    gp_xs = [gp_left_xs; gp_data_xs; gp_right_xs]\n    data_idxs = collect(gp_left_bin_offset+1:GP_GRAIN_FACTOR:gp_left_bin_offset+DATA_DIM*GP_GRAIN_FACTOR)\n    gp_xs, gp_binsize, data_idxs\nend;\nnothing #hide","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Based on the defined model properties, we generate the grid. GP grid is the fine-grained grid with offsets added to the data range.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"_GP_XS represent bin centers of such a fine-grained grid\n_GP_BINSIZE is the width of the bin (that is 1/GP_GRAIN_FACTOR of data bin size)\n_DATA_IDXS - integer indices of the left edges of the data bins","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"_GP_XS, _GP_BINSIZE, _DATA_IDXS = produce_bins();\n_GP_DIM = length(_GP_XS);\nnothing #hide","category":"page"},{"location":"advanced_tutorial/#Model-parameters-1","page":"Advanced Tutorial","title":"Model parameters","text":"","category":"section"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The Gaussian process in this tutorial is modeled in the Fourier space with zero mean and two hyperparameters defining properties of its kernel. To sample from this Gaussian process, we also need a parameter per bin that will represent the particular realization of the GP in the bin.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"function assemble_paridx(;kwargs...)\n    pos = 0\n    res = []\n    for (k, v) in kwargs\n        new_start, new_stop = v.start+pos, v.stop+pos\n        push!(res, (k, (v.start+pos):(v.stop+pos)))\n        pos = new_stop\n    end\n    (;res...)\nend;\nnothing #hide","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"MGVI is an iterative procedure, so we will need to introduce an initial guess for the state of the model. We create a vector with size equal to the count of all parameters' starting_point and a NamedTuple PARDIX that assigns names to the sub-regions in this vector. In the correct case:","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"gp_hyper is two hyperparameters of the Gaussian process stored in the first two cells of the parameter vector\ngp_latent _GP_DIM are parameters used to define the particular realization of the gaussian process,  stored at indices between 3 to 2 + _GP_DIM.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Function assemble_paridx is responsible for constructing such a NamedTuple from the parameter specification.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"PARIDX = assemble_paridx(gp_hyper=1:2, gp_latent=1:_GP_DIM);\n\nstarting_point = randn(last(PARIDX).stop);\nnothing #hide","category":"page"},{"location":"advanced_tutorial/#Model-implementation-1","page":"Advanced Tutorial","title":"Model implementation","text":"","category":"section"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"k = collect(0:(_GP_DIM)÷2 -1);\nnothing #hide","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"A Gaussian process's covariance in the Fourier space is represented with a diagonal matrix. Values on the diagonal follow a squared exponential function with parameters depending on priors. A kernel that is diagonal and mirrored around the center represents a periodic and translationally invariant function in the coordinate space. This property restricts covariance to have a finite correlation length in the coordinate space.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"MGVI assumes that all priors are distributed as standard normals N(0, 1); thus, to modify the shapes of the priors, we explicitly rescale them at the model implementation phase.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We also exponentiate each prior before using it to tune the squared exponential shape. In doing so, we ensure only positive values for the kernel's hyperparameters.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Actually, for the sake of numeric stability we model already square root of the covariance. This can be traced by missing sqrt in the next level, where we sample from the Gaussian process.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"function sqrt_kernel(p)\n    kernel_A_c, kernel_l_c = p[PARIDX.gp_hyper]\n    kernel_A = 60*exp(kernel_A_c*0.9)*GP_GRAIN_FACTOR\n    kernel_l = 0.025*exp(kernel_l_c/15)/(GP_GRAIN_FACTOR^0.3)\n    positive_modes = kernel_A .* sqrt(2 * π * kernel_l) .* exp.( -π^2 .* k.^2 .* kernel_l^2)\n    negative_modes = positive_modes[end:-1:1]\n    [positive_modes; negative_modes]\nend;\nnothing #hide","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"As a Fourier transform we choose the Discrete Hartley Transform, which ensures that Fourier coefficients of the real valued function remain real valued.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"ht = FFTW.plan_r2r(zeros(_GP_DIM), FFTW.DHT);\nnothing #hide","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Before we proceed, let's have a brief look at the kernel's shape. Below we plot the kernel in the coordinate space K(r) = K(x2 - x1) as a function of time in years between two points. As we go further along the x-axis, the time interval will increase, and the covariance will decrease.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"function plot_kernel_model(p, width; plot_args=(;))\n    xs = collect(1:Int(floor(width/_GP_BINSIZE)))\n    plot!(xs .* _GP_BINSIZE, (ht * (sqrt_kernel(p) .^ 2))[xs] ./ _GP_DIM, label=nothing, linewidth=2.5; plot_args...)\nend\n\nplot()\nplot_kernel_model(starting_point, 20)","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To make it even more visual, we also plot the structure of the covariance matrix as a heatmap. We see that the finite correlation length shows up as a band around the diagonal. We also see small artifacts in the antidiagonal corners. These come from the assumption that the kernel is periodic.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"function plot_kernel_matrix(p)\n    xkernel = ht * (sqrt_kernel(p) .^ 2) ./ _GP_DIM\n    res = reduce(hcat, [circshift(xkernel, i) for i in 0:(_GP_DIM-1)])'\n    heatmap!(_GP_XS, _GP_XS, res; yflip=true, xmirror=true, tick_direction=:out, top_margin=20px, right_margin=30px)\nend\n\nplot()\nplot_kernel_matrix(starting_point)","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"After we defined the square root of the kernel function (sqrt_kernel), we just follow the regular procedure of sampling from the normal distribution. Since the covariance matrix in the Fourier space is diagonal, Gaussian variables in each bin are independent of each other. Thus, sampling ends up rescaling the gp_latent part of the prior vector responsible for the Gaussian process state.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"After we produced a sample of Gaussian random values following the kernel model, we apply a Fourier transform to return back to the coordinate space.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"function gp_sample(p)\n    flat_gp = sqrt_kernel(p) .* p[PARIDX.gp_latent]\n    (ht * flat_gp) ./ _GP_DIM\nend;\nnothing #hide","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Together with the implementation of gp_sample we also need to define its version of the Duals. This will allow our application of the Hartley transform to be differentiatiable.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"function gp_sample(dp::Vector{ForwardDiff.Dual{T, V, N}}) where {T,V,N}\n    flat_gp_duals = sqrt_kernel(dp) .* dp[PARIDX.gp_latent]\n    val_res = ht*ForwardDiff.value.(flat_gp_duals) ./ _GP_DIM\n    psize = size(ForwardDiff.partials(flat_gp_duals[1]), 1)\n    ps = x -> ForwardDiff.partials.(flat_gp_duals, x)\n    val_ps = map((x -> ht*ps(x) ./ _GP_DIM), 1:psize)\n    ForwardDiff.Dual{T}.(val_res, val_ps...)\nend;\nnothing #hide","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Gaussian process realization is meant to serve as a Poisson rate of the Poisson process. Since the Gaussian process is not restricted to positive values, we exponentiate its values to forcefully make the function positive.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"function poisson_gp_link(fs)\n    exp.(fs)\nend;\nnothing #hide","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Now when we have a function representing the Poisson rate density, we have to integrate it over each data bin to define the Poisson rate in these bins. Function agg_lambdas does precisely that. When GP_GRAIN_FACTOR = 1, this function just multiplies the value of the Gaussian process in the bin by the _GP_BINSIZE. When we have more GP bins per data bin (GP_GRAIN_FACTOR > 1), then we apply rectangular quadrature to integrate over the bin.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"function _forward_agg(data, idxs, steps_forward)\n    [sum(data[i:i+steps_forward-1]) for i in idxs]\nend;\n\nfunction agg_lambdas(lambdas)\n    gps = _forward_agg(lambdas, _DATA_IDXS, GP_GRAIN_FACTOR) .* _GP_BINSIZE\n    xs = _GP_XS[_DATA_IDXS .+ (GP_GRAIN_FACTOR ÷ 2)]\n    xs, gps\nend;\nnothing #hide","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Finally, we define the model by using the building blocks defined above:","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"gp_sample sample from the Gaussian process with defined sqrt_kernel covariance\npoisson_gp_link ensures Gaussian process is positive\nagg_lambdas integrates Gaussian process over each data bin to turn it into a Poisson rate for each bin\nmodel maps parameters into the product of the Poisson distribution's counting events in each bin.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"function model(params)\n    fs = gp_sample(params)\n    fine_lambdas = poisson_gp_link(fs)\n    _, lambdas = agg_lambdas(fine_lambdas)\n    Product(Poisson.(lambdas))\nend;\nnothing #hide","category":"page"},{"location":"advanced_tutorial/#Visualization-utilities-1","page":"Advanced Tutorial","title":"Visualization utilities","text":"","category":"section"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"function agg_full_lambdas(lambdas)\n    left_idxs = 1:GP_GRAIN_FACTOR:(_DATA_IDXS[1]-GP_GRAIN_FACTOR)\n    left_gp = _forward_agg(lambdas, left_idxs, GP_GRAIN_FACTOR) .* _GP_BINSIZE\n    left_xs = _GP_XS[left_idxs .+ (GP_GRAIN_FACTOR ÷ 2)]\n    right_idxs = (_DATA_IDXS[end]+1):GP_GRAIN_FACTOR:(size(lambdas, 1) - GP_GRAIN_FACTOR)\n    right_gp = _forward_agg(lambdas, right_idxs, GP_GRAIN_FACTOR) .* _GP_BINSIZE\n    right_xs = _GP_XS[right_idxs .+ (GP_GRAIN_FACTOR ÷ 2)]\n    middle_xs, middle_gp = agg_lambdas(lambdas)\n    full_xs = [left_xs; middle_xs; right_xs]\n    full_gp = [left_gp; middle_gp; right_gp]\n    full_xs, full_gp\nend;\n\nfunction _mean(p; full=false)\n    agg_func = if (!full) agg_lambdas else agg_full_lambdas end\n    xs, gps = agg_func(poisson_gp_link(gp_sample(p)))\n    xs, gps\nend;\n\nfunction plot_mean(p, label=\"mean\"; plot_args=(;), full=false)\n    plot!(_mean(p; full=full)...; label=label, linewidth=3, plot_args...)\nend;\n\nfunction plot_prior_samples(num_samples; mean_plot_args=(;))\n    for _ in 1:num_samples\n        p = randn(last(PARIDX).stop)\n        plot_mean(p, nothing; plot_args=mean_plot_args)\n    end\nend;\n\nfunction plot_kernel_prior_samples(num_samples, width)\n    for _ in 1:num_samples\n        p = randn(last(PARIDX).stop)\n        plot_kernel_model(p, width)\n    end\n    plot!()\nend;\n\nfunction plot_data(; scatter_args=(;), smooth_args=(;))\n    bar!(_GP_XS[_DATA_IDXS .+ (GP_GRAIN_FACTOR ÷ 2)], data, color=:deepskyblue2, la=0, markersize=2., markerstrokewidth=0, alpha=0.4, label=\"data\"; scatter_args...)\n    smooth_step = 4\n    smooth_xs = _GP_XS[_DATA_IDXS .+ (GP_GRAIN_FACTOR ÷ 2)][(smooth_step+1):(end-smooth_step)]\n    smooth_data = [sum(data[i-smooth_step:i+smooth_step])/(2*smooth_step+1) for i in (smooth_step+1):(size(data, 1)-smooth_step)]\n    plot!(smooth_xs, smooth_data, color=:deeppink3, linewidth=3, linealpha=1, ls=:dash, label=\"smooth data\"; smooth_args...)\nend;\n\nfunction plot_mgvi_samples(samples)\n    for sample in eachcol(samples)\n        if any(isnan.(sample))\n            print(\"nan found in samples\", \"\\n\")\n            continue\n        end\n        plot!(_mean(Vector(sample))..., linealpha=0.5, linewidth=2, label=nothing)\n    end\n    plot!()\nend;\n\nfunction plot_kernel_mgvi_samples(samples, width)\n    for sample in eachcol(samples)\n        if any(isnan.(sample))\n            print(\"nan found in samples\", \"\\n\")\n            continue\n        end\n        plot_kernel_model(sample, width; plot_args=(linealpha=0.5, linewidth=2, label=nothing))\n    end\n    plot!()\nend;\n\nfunction produce_posterior_samples(p, num_residuals)\n    batch_size = 10\n\n    if num_residuals <= 2*batch_size\n        batch_size = num_residuals ÷ 2\n    end\n\n    est_res_sampler = MGVI._create_residual_sampler(model, p;\n                                                    residual_sampler=ImplicitResidualSampler,\n                                                    jacobian_func=FwdRevADJacobianFunc,\n                                                    residual_sampler_options=(;cg_params=(;abstol=1E-2)))\n    batches = []\n    for _ in 1:(num_residuals ÷ batch_size ÷ 2)\n        batch_residual_samples = MGVI.rand(Random.GLOBAL_RNG, est_res_sampler, batch_size)\n        push!(batches, p .+ batch_residual_samples)\n        push!(batches, p .- batch_residual_samples)\n    end\n    reduce(hcat, batches)\nend\n\nfunction _extract_quantile(sorted_gp_realizations, p)\n    map(s -> quantile(s, p; sorted=true), eachrow(sorted_gp_realizations))\nend;\n\nfunction plot_posterior_bands(p, num_samples; full=false)\n    bands = [(0.997, :red), (0.955, :goldenrod1), (0.683, :green)]\n    samples = produce_posterior_samples(p, num_samples)\n    xs, first_gp = _mean(samples[1:end, 1]; full=full)\n    gp_realizations = reduce(hcat, [_mean(Vector(sample); full=full)[2] for sample in eachcol(samples[1:end, 2:end])]; init=first_gp)\n    for (i, one_x_sample) in enumerate(eachrow(gp_realizations))\n        gp_realizations[i, 1:end] .= sort(Vector(one_x_sample))\n    end\n    for (band, color) in bands\n        quant_l = _extract_quantile(gp_realizations, (1-band)/2)\n        quant_u = _extract_quantile(gp_realizations, (1+band)/2)\n        plot!(xs, quant_l; fillrange=quant_u, fillcolor=color, linealpha=0, label=band)\n    end\n    sample_median = _extract_quantile(gp_realizations, 0.5)\n    plot!(xs, sample_median; linewidth=2, linecolor=:grey25, label=\"median\")\nend;\nnothing #hide","category":"page"},{"location":"advanced_tutorial/#Visualization-and-fitting-1","page":"Advanced Tutorial","title":"Visualization and fitting","text":"","category":"section"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We start by plotting the Gaussian process's dynamic range by sampling many possible realizations of it unconditionally on the data. We expect the set of lines to populate the regions where there are data.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"plot()\nplot_data(;scatter_args=(;alpha=0.7))\nplot_prior_samples(200, mean_plot_args=(;alpha=0.5))\nplot!(ylim=[0, 8])","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We also plot prior samples for the kernel in the coordinate space. The plot below shows that the kernel is flexible in the amplitude while the correlation length is quite strongly predefined:","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"plot()\nplot_kernel_prior_samples(200, 20)","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Now that we see that the Gaussian process is potentially able to fit the data, we plot the initial guess (starting_point) to see where we should start from. This plot shows:","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"data points\nsmoothed data with a moving average of 9 years\nPoisson rate for each bin\nMGVI samples around the mean. At the later stages they can be used to estimate MGVI's uncertainty","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"plot()\nplot_data()\nplot_mean(starting_point, \"starting point\"; plot_args=(;color=:darkorange2))\nplot_mgvi_samples(produce_posterior_samples(starting_point, 6))","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We also want to introduce the full plot that shows not only the data region, but includes the region with the padding we added with GP_PADDING. We will use this plot to make sure that periodic boundary conditions don't interfere with the data.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"plot()\nplot_data()\nplot_mean(starting_point, \"full gp\"; full=true, plot_args=(;color=:pink))\nplot_mean(starting_point, \"starting point\"; plot_args=(;color=:darkorange2))","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Below we also plot the kernel and MGVI samples that represent the possible variation of the kernel shape around the mean:","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"plot()\nplot_kernel_model(starting_point, 20; plot_args=(;label=\"kernel model\"))\nplot_kernel_mgvi_samples(produce_posterior_samples(starting_point, 6), 20)","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Let's make a first iteration of the MGVI. For purposes of displaying the convergence curve, we limit Optim.option to 1 iteration so that MGVI will coverge more slowly.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"first_iteration = mgvi_kl_optimize_step(Random.GLOBAL_RNG,\n                                        model, data,\n                                        starting_point;\n                                        num_residuals=3,\n                                        jacobian_func=FwdRevADJacobianFunc,\n                                        residual_sampler=ImplicitResidualSampler,\n                                        optim_options=Optim.Options(iterations=1, show_trace=false),\n                                        residual_sampler_options=(;cg_params=(;abstol=1E-2,verbose=false)));\nnothing #hide","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We again plot data and the Poisson rate. We again show the Gaussian process with padding. After one iteration the Poisson rate doesn't seem to get much closer to the data.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"plot()\nplot_data()\nplot_mean(first_iteration.result, \"first iteration\"; plot_args=(;color=:darkorange2))\nplot_mgvi_samples(first_iteration.samples)","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"plot()\nplot_data()\nplot_mean(first_iteration.result, \"full gp\"; full=true, plot_args=(;color=:pink))\nplot_mean(first_iteration.result, \"first iteration\"; plot_args=(;color=:darkorange2))","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Kernel and its MGVI samples changed significantly in comparison to the starting_point even after the first iteration:","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"plot()\nplot_kernel_model(first_iteration.result, 20; plot_args=(;label=\"kernel model\"))\nplot_kernel_mgvi_samples(first_iteration.samples, 20)","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In order to visualize convergence we prepare a few functions to compute, store and plot the average posterior likelihood of.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"function compute_avg_likelihood(model, samples, data)\n    tot = 0\n    for sample in eachcol(samples)\n        tot += -MGVI.posterior_loglike(model, sample, data)\n    end\n    tot/size(samples, 2)\nend;\n\nfunction show_avg_likelihood(series)\n    scatter!(1:size(series, 1), series, label=\"-loglike\")\nend;\nnothing #hide","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Now we do 30 more iterations of the MGVI and store the average likelihood after each step. We feed the fitted result of the previous step as an input to the next iteration.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"next_iteration = first_iteration;\navg_likelihood_series = [];\npush!(avg_likelihood_series, compute_avg_likelihood(model, next_iteration.samples, data));\nfor i in 1:30\n    tmp_iteration = mgvi_kl_optimize_step(Random.GLOBAL_RNG,\n                                          model, data,\n                                          next_iteration.result;\n                                          num_residuals=3,\n                                          jacobian_func=FwdRevADJacobianFunc,\n                                          residual_sampler=ImplicitResidualSampler,\n                                          optim_options=Optim.Options(iterations=1, show_trace=false),\n                                          residual_sampler_options=(;cg_params=(;abstol=1E-2,verbose=false)))\n    global next_iteration = tmp_iteration\n    push!(avg_likelihood_series, compute_avg_likelihood(model, next_iteration.samples, data))\nend;\nnothing #hide","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"First, let's have a look at the convergence plots. We see that MGVI converged after 10 iterations while being limited to very poor Optim performance.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"plot(yscale=:log)\nshow_avg_likelihood(avg_likelihood_series)","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Below we plot the result of the fit. Together with the data and Poisson rate, we also plot MGVI residuals. These are samples from the Gaussian posterior, sampled with respect to the posterior's covariance structure. Thus MGVI residual samples are deviations from the MGVI fit and represent how confident we are about the prediction.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"plot(ylim=[0,8])\nplot_data()\nplot_mgvi_samples(next_iteration.samples)\nplot_mean(next_iteration.result, \"many iterations\"; plot_args=(;color=:darkorange2))","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To present credibility intervals we also plot credibility bands. We sample 400 residual samples from MGVI and then plot quantiles for each data bin. This should give us a feeling of how compatible the MGVI fit is with the data.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"plot(ylim=[0,8])\nplot_posterior_bands(next_iteration.result, 400)\nplot_data()\nplot_mean(next_iteration.result, \"many iterations\"; plot_args=(;color=:darkorange2))","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We also make sure boundary conditions do not interfere with the data. Here is the Gaussian process plot with the paddings included:","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"plot()\nplot_data()\nplot_mean(next_iteration.result; full=true, plot_args=(;color=:pink))\nplot_mean(next_iteration.result, \"many iterations\"; plot_args=(;color=:darkorange2))","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Let's have a look at the kernel again. We expect the variation of samples to become narrower:","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"plot()\nplot_kernel_model(next_iteration.result, 20; plot_args=(;label=\"kernel model\"))\nplot_kernel_mgvi_samples(next_iteration.samples, 20)","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"advanced_tutorial/#Maximum-a-posteriori-estimation-1","page":"Advanced Tutorial","title":"Maximum a posteriori estimation","text":"","category":"section"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We build a MAP as a cross check of MGVI results. We simply optimize the posterior likelihood by using Optim without any particular tuning settings:","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"max_posterior = Optim.optimize(x -> -MGVI.posterior_loglike(model, x, data), starting_point, LBFGS(), Optim.Options(show_trace=false, g_tol=1E-10, iterations=300));\nnothing #hide","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We observe that the bump in the middle (around 1910) is caught by the MAP while it is less pronounced in the MGVI fit. MAP also has finer structure around 1875 and 1835.","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"plot()\nplot_data()\nplot_mean(next_iteration.result, \"mgvi mean\"; plot_args=(;color=:darkorange2))\nplot_mean(Optim.minimizer(max_posterior), \"map\")","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We also can see the difference at the left edge of the data region. While MGVI smoothed the data, the MAP predicted a consequent peak:","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"plot()\nplot_data()\nplot_mean(Optim.minimizer(max_posterior), \"full gp\"; full=true, plot_args=(;color=:darkorange2))\nplot_mean(next_iteration.result, \"mgvi full gp\"; full=true)","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"","category":"page"},{"location":"advanced_tutorial/#","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#MGVI.jl-1","page":"Home","title":"MGVI.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"MGVI is an iterative method that performs a series of Gaussian approximations to the posterior. We alternate between approximating the covariance with the inverse Fisher information metric evaluated at an intermediate mean estimate and optimizing the KL-divergence for the given covariance with respect to the mean. This procedure is iterated until the uncertainty estimate is self-consistent with the mean parameter. We achieve linear scaling by avoiding to store the covariance explicitly at any time. Instead we draw samples from the approximating distribution relying on an implicit representation and numerical schemes to approximately solve linear equations. Those samples are used to approximate the KL-divergence and its gradient. The usage of natural gradient descent allows for rapid convergence. Formulating the Bayesian model in standardized coordinates makes MGVI applicable to any inference problem with continuous parameters. ","category":"page"},{"location":"#Citing-MGVI.jl-1","page":"Home","title":"Citing MGVI.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"When using MGVI.jl for research, teaching or similar, please cite this publication: Metric Gaussian Variational Inference","category":"page"},{"location":"#","page":"Home","title":"Home","text":"@article{knollmüller2020metric,\n         title={Metric Gaussian Variational Inference},\n         author={Jakob Knollmüller and Torsten A. Enßlin},\n         year={2020},\n         eprint={1901.11033},\n         archivePrefix={arXiv},\n         primaryClass={stat.ML}\n}","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"EditURL = \"https://github.com/bat/MGVI.jl/blob/master/docs/build/src/tutorial_lit.jl\"","category":"page"},{"location":"tutorial/#Tutorial-1","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Notebook download nbviewer source","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using MGVI","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using Distributions\nusing Random\nusing ValueShapes\nusing LinearAlgebra\nusing Optim","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"We want to fit a 3-degree polynomial using two data sets (a and b). MGVI requires a model expressed as a function of the model parameters and returning an instance of the Distribution. In this example, since we have two sets of independent measurements, we express them as ValueShapes.NamedTupleDist.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"We assume errors are normally distributed with unknown covariance, which has to be learned as well.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"const _x1_grid = [Float64(i)/10 for i in 1:25]\nconst _x2_grid = [Float64(i)/10 + 0.1 for i in 1:15]\nconst _common_grid = sort(vcat(_x1_grid, _x2_grid))\n\nfunction _mean(x_grid, p)\n    p[1]*10 .+ p[2]*40 .* x_grid .+ p[3]*600 .* x_grid.^2 .+ p[4]*80 .* x_grid.^3\nend\n\nfunction model(p)\n    dist1 = Product(Normal.(_mean(_x1_grid, p), p[5]^2*60))\n    dist2 = Product(Normal.(_mean(_x2_grid, p), p[5]^2*60))\n    NamedTupleDist(a=dist1,\n                   b=dist2)\nend","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"model (generic function with 1 method)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Here we define the ground truth of the parameters, as well as an initial guess.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"const true_params =  [\n -0.3\n -1.5\n 0.2\n -0.5\n 0.3]\n\nconst starting_point = [\n  0.2\n  0.5\n  -0.1\n  0.3\n -0.6\n];\nnothing #hide","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"function pprintln(obj)\n    show(stdout, \"text/plain\", obj)\n    println()\nend;\nnothing #hide","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using Plots\ngr(size=(400, 300), dpi=700, fmt=:png)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Plots.GRBackend()","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"rng = MersenneTwister(157);\nnothing #hide","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"We draw data directly from the model, using the true parameter values:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"data = rand(rng, model(true_params), 1)[1];\nnothing #hide","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"function _mean(x::Vector)\n    _mean(_common_grid, x)\nend\n\ninit_plots =() -> let\n    truth = _mean(true_params)\n    plot!(_common_grid, truth, markercolor=:blue, linecolor=:blue, label=\"truth\")\n    scatter!(_common_grid, _mean(starting_point), markercolor=:orange, markerstrokewidth=0, markersize=3, label=\"init\")\n    scatter!(vcat(_x1_grid, _x2_grid), reduce(vcat, data), markercolor=:black, markerstrokewidth=0, markersize=3, label=\"data\")\nend;\nnothing #hide","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Before we start the optimization, let's have an initial look at the data. It is also interesting to see how our starting guess performs.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"p = plot()\ninit_plots()","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Now we are ready to run one iteration of the MGVI. The output contains an updated parameter estimate (first_iteration.result), which we can compare to the true parameters.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"first_iteration = mgvi_kl_optimize_step(rng,\n                                        model, data,\n                                        starting_point;\n                                        jacobian_func=FwdRevADJacobianFunc,\n                                        residual_sampler=ImplicitResidualSampler,\n                                        optim_options=Optim.Options(iterations=10, show_trace=true),\n                                        residual_sampler_options=(;cg_params=(;maxiter=10)))\npprintln(hcat(first_iteration.result, true_params))\np","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"plot_iteration = (params, label) -> let\n    #error_mat = mgvi_kl_errors(full_model, params)\n    #display(error_mat)\n    #errors = sqrt.(error_mat[diagind(error_mat)])\n    #yerr = abs.(line(common_grid, params+errors) - line(common_grid, params-errors))\n    #scatter!(common_grid, line(common_grid, params), markercolor=:green, label=label, yerr=yerr)\n    for sample in eachcol(params.samples)\n        scatter!(_common_grid, _mean(Vector(sample)), markercolor=:gray, markeralpha=0.3, markersize=2, label=nothing)\n    end\n    scatter!(_common_grid, _mean(params.result), markercolor=:green, label=label)\nend;\nnothing #hide","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Now let's also plot the curve corresponding to the new parameters after the first iteration:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"p = plot()\ninit_plots()\nplot_iteration(first_iteration, \"first\")\np","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"plot_iteration_light = (params, counter) -> let\n    scatter!(_common_grid, _mean(params.result), markercolor=:green, markersize=3, markeralpha=2*atan(counter/18)/π, label=nothing)\nend;\nnothing #hide","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"From the plot above we see that one iteration is not enough. Let's do 5 more steps and plot the evolution of estimates.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"init_plots()\nplt = scatter()\nnext_iteration = first_iteration\nfor i in 1:5\n    pprintln(minimum(next_iteration.optimized))\n    pprintln(hcat(next_iteration.result, true_params))\n    global next_iteration = mgvi_kl_optimize_step(rng,\n                                                  model, data,\n                                                  next_iteration.result;\n                                                  jacobian_func=FwdRevADJacobianFunc,\n                                                  residual_sampler=ImplicitResidualSampler,\n                                                  optim_options=Optim.Options(iterations=10, show_trace=true),\n                                                  residual_sampler_options=(;cg_params=(;maxiter=10)))\n    plot_iteration_light(next_iteration, i)\nend\npprintln(minimum(next_iteration.optimized))\npprintln(hcat(next_iteration.result, true_params))\nplt","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Finally, let's plot the last estimate and compare it to the truth. Also, notice, that gray dots represent samples from the approximation.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"p = plot()\ninit_plots()\nplot_iteration(next_iteration, \"last\")\np","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"This page was generated using Literate.jl.","category":"page"}]
}
