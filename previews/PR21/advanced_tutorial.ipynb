{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this tutorial we will fit coal mining disaster dataset with a Gaussian process modulated\n",
    "Poisson process."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare environment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by importing:\n",
    "* MGVI that we will use for posterior fit\n",
    "* Distributions and FFTW to define the statistical model\n",
    "* Optim to pass Optim.Options to MGVI and to find Maximum A-Posteriori fit that we will use for comparison\n",
    "* StatsBase for histogram preparation from the data and also for error bands visualization\n",
    "* Plots.jl for visualization"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MGVI\n",
    "\n",
    "using Distributions\n",
    "using DelimitedFiles\n",
    "using Random\n",
    "using Optim\n",
    "using StatsBase\n",
    "\n",
    "using Plots\n",
    "\n",
    "using FFTW\n",
    "\n",
    "import ForwardDiff"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Random.seed!(84612);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataset is attached to the repository and contains intervals in days between\n",
    "disasters happend at british coal mines between March 1851 and March 1962.\n",
    "We split the entire time range into intervals of 365 days, then number of events\n",
    "in each interval costitute the measurement we are going to model."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function read_coal_mining_data(filepath, binsize)\n",
    "    init_year = empty\n",
    "    data = empty\n",
    "    open(filepath) do io\n",
    "        raw = readline(io)\n",
    "        while ! occursin(\"init_date\", raw)\n",
    "            raw = readline(io)\n",
    "        end\n",
    "\n",
    "        init_year = parse(Float64, split(split(strip(raw[2:end]), \"\\t\")[2], \"-\")[1])\n",
    "        data = readdlm(io, '\\t', Int, '\\n', comments=true)[:]\n",
    "    end\n",
    "    dates_fract_years = init_year .+ cumsum(data)/365\n",
    "    left_edge = dates_fract_years[1]\n",
    "    num_bins = ((dates_fract_years[end] - left_edge) ÷ binsize)\n",
    "    right_edge = left_edge + binsize*num_bins\n",
    "    fit(Histogram, dates_fract_years, left_edge:binsize:right_edge).weights\n",
    "end\n",
    "\n",
    "coal_mine_disaster_data = read_coal_mining_data(joinpath(@__DIR__, \"coal_mining_data.tsv\"), 1);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Global parameters and the grid"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we define several model properties:\n",
    "* `DATA_DIM` is just a size of the dataset\n",
    "* `DATA_XLIM` specifies the time range of the data\n",
    "* `GP_GRAIN_FACTOR` determines numbers of finer bins into which data bin is split.\n",
    "   This is useful when there are several datasets defined on different grids.\n",
    "* `GP_PADDING` adds empty paddings to the dataset. We use Fourier transform to sample from the Gaussian process\n",
    "   with a finite correlation length. `GP_PADDING` helps us to ensure that periodic boundary conditions\n",
    "   imposed by Fourier transform won't affect data region."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "DATA_DIM = size(coal_mine_disaster_data, 1);\n",
    "\n",
    "data = coal_mine_disaster_data;\n",
    "\n",
    "DATA_XLIM = [1851., 1962.];\n",
    "\n",
    "GP_GRAIN_FACTOR = 3;\n",
    "GP_PADDING = 80;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function produce_bins()\n",
    "    data_binsize = (DATA_XLIM[2] - DATA_XLIM[1])/DATA_DIM\n",
    "    gp_binsize = data_binsize/GP_GRAIN_FACTOR\n",
    "    gp_dim = Integer(((DATA_XLIM[2] - DATA_XLIM[1]) + 2*GP_PADDING) ÷ gp_binsize)\n",
    "    gp_left_bin_offset = gp_right_bin_offset = (gp_dim - DATA_DIM) ÷ 2\n",
    "    if (2*gp_left_bin_offset + DATA_DIM*GP_GRAIN_FACTOR) % 2 == 1\n",
    "        gp_left_bin_offset += 1\n",
    "    end\n",
    "    gp_left_xlim = DATA_XLIM[1] - gp_left_bin_offset*gp_binsize\n",
    "    gp_right_xlim = DATA_XLIM[2] + gp_right_bin_offset*gp_binsize\n",
    "    gp_left_xs = collect(gp_left_xlim + gp_binsize/2:gp_binsize:DATA_XLIM[1])\n",
    "    gp_right_xs = collect(DATA_XLIM[2] + gp_binsize/2:gp_binsize:gp_right_xlim)\n",
    "    gp_data_xs = collect(DATA_XLIM[1] + gp_binsize/2:gp_binsize:DATA_XLIM[2])\n",
    "    gp_xs = [gp_left_xs; gp_data_xs; gp_right_xs]\n",
    "    data_idxs = collect(gp_left_bin_offset+1:GP_GRAIN_FACTOR:gp_left_bin_offset+DATA_DIM*GP_GRAIN_FACTOR)\n",
    "    gp_xs, gp_binsize, data_idxs\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the defined model properties we generate the grid. GP grid is the fine grained grid\n",
    "with offsets added to the data range.\n",
    "* `_GP_XS` represent bin centers of such a fine grained grid\n",
    "* `_GP_BINSIZE` is the width of the bin (that is 1/`GP_GRAIN_FACTOR` of data bin size)\n",
    "* `_DATA_IDXS` - integer indices of the left edges of the data bins"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "_GP_XS, _GP_BINSIZE, _DATA_IDXS = produce_bins();\n",
    "_GP_DIM = length(_GP_XS);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gaussian process in this tutorial is modeled in the Fourier space with zero mean\n",
    "and two hyperparameters defining properties of its kernel. To sample from this\n",
    "Gaussian process we also need a parameter per bin that will represent the particular\n",
    "realisation of the GP in the bin"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function assemble_paridx(;kwargs...)\n",
    "    pos = 0\n",
    "    res = []\n",
    "    for (k, v) in kwargs\n",
    "        new_start, new_stop = v.start+pos, v.stop+pos\n",
    "        push!(res, (k, (v.start+pos):(v.stop+pos)))\n",
    "        pos = new_stop\n",
    "    end\n",
    "    (;res...)\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "MGVI is an iterative procedure, so we will need to introduce an initial guess for the state of the model.\n",
    "For this we create one vector of the size of the count of all parameters `starting_point` and a NamedTuple\n",
    "`PARDIX` that assignes names to the sub-regions in the vector of parameters. In the currect case:\n",
    "* `gp_hyper` two hyperparameters of the Gaussian process stored in the first two cells of the parameter vector\n",
    "* `gp_latent` `_GP_DIM` parameters used to define the particular realization of the gaussian process,\n",
    "stored at indices between `3` to `2 + _GP_DIM`.\n",
    "\n",
    "Function `assemble_paridx` is responsible for constructing such a NamedTuple from the parameter specification."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "PARIDX = assemble_paridx(gp_hyper=1:2, gp_latent=1:_GP_DIM);\n",
    "\n",
    "starting_point = randn(last(PARIDX).stop);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model implementation"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "k = collect(0:(_GP_DIM)÷2 -1);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gaussian process covariance in the fourier space is represented with a diagonal matrix. Values\n",
    "on the diagonal follow squared exponential function with parameters depending on priors.\n",
    "Diagonal kernel that is mirrored around the center represents periodic and translation invariant function\n",
    "in the coordinate space. This helps to ensure that covariance has finite correlation length in the coordinate\n",
    "space.\n",
    "\n",
    "MGVI assumes that all priors are distributed as standard normals `N(0, 1)`, thus in order\n",
    "to modify shapes of the priors we explicitly rescale them at the model implementation phase.\n",
    "\n",
    "We also exponentiate each prior before using it for tuning squared exponential shape. In this way\n",
    "we impose only positive values of the hyperparameters of the kernel.\n",
    "\n",
    "Actually, for the sake of numeric stability we model already square root of the covariance.\n",
    "This can be traced by missing `sqrt` in the next level, where we sample from the Gaussian process."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function kernel_model(p)\n",
    "    kernel_A_c, kernel_l_c = p[PARIDX.gp_hyper]\n",
    "    kernel_A = 60*exp(kernel_A_c*0.9)*GP_GRAIN_FACTOR\n",
    "    kernel_l = 0.025*exp(kernel_l_c/15)/(GP_GRAIN_FACTOR^0.3)\n",
    "    positive_modes = kernel_A .* sqrt(2 * π * kernel_l) .* exp.( -π^2 .* k.^2 .* kernel_l^2)\n",
    "    negative_modes = positive_modes[end:-1:1]\n",
    "    [positive_modes; negative_modes]\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a Fourier transform we use Discrete Hartley Transform that ensures that Fourier\n",
    "coefficients of the real valued function are real valued."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ht = FFTW.plan_r2r(zeros(_GP_DIM), FFTW.DHT);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "After we defined the square root of the kernel function (`kernel_model`)\n",
    "we just follow the regular procedure of sampling from the normal distribution.\n",
    "Since the covariance matrix in the Fourier space is diagonal, Gaussian variables\n",
    "in each bin are independent of each other. Thus, sampling end up in rescaling\n",
    "priors responsible for the Gaussian process state `gp_latent`.\n",
    "\n",
    "After we produced a sample of Gaussian random values following the kernel model,\n",
    "we apply Fourier transform to return back to the coordinate space."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function gp_sample(p)\n",
    "    flat_gp = kernel_model(p) .* p[PARIDX.gp_latent]\n",
    "    (ht * flat_gp) ./ _GP_DIM\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Together with the implementation of `gp_sample` we also need\n",
    "it's version for the `Dual`s. It is a little patch that makes\n",
    "application of the Hartley transform differentiatable."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function gp_sample(dp::Vector{ForwardDiff.Dual{T, V, N}}) where {T,V,N}\n",
    "    flat_gp_duals = kernel_model(dp) .* dp[PARIDX.gp_latent]\n",
    "    val_res = ht*ForwardDiff.value.(flat_gp_duals) ./ _GP_DIM\n",
    "    psize = size(ForwardDiff.partials(flat_gp_duals[1]), 1)\n",
    "    ps = x -> ForwardDiff.partials.(flat_gp_duals, x)\n",
    "    val_ps = map((x -> ht*ps(x) ./ _GP_DIM), 1:psize)\n",
    "    ForwardDiff.Dual{T}.(val_res, val_ps...)\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gaussian process realisation is meant to serve as a Poisson rate of the Poisson\n",
    "process. Since Gaussian process is not restricted to positive values, we\n",
    "exponentiate its values to forcefully make the function positive."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function poisson_gp_link(fs)\n",
    "    exp.(fs)\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now when we have a function representing the density of the Poisson rate,\n",
    "we have to integrate it over each data bin to define Poisson rate in these bins.\n",
    "Function `agg_lambdas` does exactly that. When `GP_GRAIN_FACTOR = 1`, this function\n",
    "just multiplies the value of the Gaussan process in the bin by the `_GP_BINSIZE`.\n",
    "When we have more GP bins per data bin (`GP_GRAIN_FACTOR > 1`) then we apply\n",
    "rectangular quadrature to integrate over the bin."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function _forward_agg(data, idxs, steps_forward)\n",
    "    [sum(data[i:i+steps_forward-1]) for i in idxs]\n",
    "end;\n",
    "\n",
    "function agg_lambdas(lambdas)\n",
    "    gps = _forward_agg(lambdas, _DATA_IDXS, GP_GRAIN_FACTOR) .* _GP_BINSIZE\n",
    "    xs = _GP_XS[_DATA_IDXS .+ (GP_GRAIN_FACTOR ÷ 2)]\n",
    "    xs, gps\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally we arrive to the model definition assembled from the building blocks we defined above:\n",
    "* `gp_sample` sample from the Gaussian process with defined `kernel_model` covariance\n",
    "* `poisson_gp_link` ensures Gaussian process is positive\n",
    "* `agg_lambdas` integrates Gaussian process over each data bin to turn it into Poisson rate for each bin\n",
    "* Model maps parameters into the product of the Poisson distributions counting events in each bin."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function model(params)\n",
    "    fs = gp_sample(params)\n",
    "    fine_lambdas = poisson_gp_link(fs)\n",
    "    _, lambdas = agg_lambdas(fine_lambdas)\n",
    "    Product(Poisson.(lambdas))\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization utilities"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function agg_full_lambdas(lambdas)\n",
    "    left_idxs = 1:GP_GRAIN_FACTOR:(_DATA_IDXS[1]-GP_GRAIN_FACTOR)\n",
    "    left_gp = _forward_agg(lambdas, left_idxs, GP_GRAIN_FACTOR) .* _GP_BINSIZE\n",
    "    left_xs = _GP_XS[left_idxs .+ (GP_GRAIN_FACTOR ÷ 2)]\n",
    "    right_idxs = (_DATA_IDXS[end]+1):GP_GRAIN_FACTOR:(size(lambdas, 1) - GP_GRAIN_FACTOR)\n",
    "    right_gp = _forward_agg(lambdas, right_idxs, GP_GRAIN_FACTOR) .* _GP_BINSIZE\n",
    "    right_xs = _GP_XS[right_idxs .+ (GP_GRAIN_FACTOR ÷ 2)]\n",
    "    middle_xs, middle_gp = agg_lambdas(lambdas)\n",
    "    full_xs = [left_xs; middle_xs; right_xs]\n",
    "    full_gp = [left_gp; middle_gp; right_gp]\n",
    "    full_xs, full_gp\n",
    "end;\n",
    "\n",
    "function _mean(p; full=false)\n",
    "    agg_func = if (!full) agg_lambdas else agg_full_lambdas end\n",
    "    xs, gps = agg_func(poisson_gp_link(gp_sample(p)))\n",
    "    xs, gps\n",
    "end;\n",
    "\n",
    "function plot_mean(p, label=\"mean\"; plot_args=(;), full=false)\n",
    "    plot!(_mean(p; full=full)..., label=label, linewidth=2; plot_args...)\n",
    "end;\n",
    "\n",
    "function plot_prior_samples(num_samples)\n",
    "    for _ in 1:num_samples\n",
    "        p = randn(last(PARIDX).stop)\n",
    "        plot_mean(p, nothing)\n",
    "    end\n",
    "end;\n",
    "\n",
    "function plot_data(; scatter_args=(;), smooth_args=(;))\n",
    "    scatter!(_GP_XS[_DATA_IDXS .+ (GP_GRAIN_FACTOR ÷ 2)], data, la=0, markersize=2., markerstrokewidth=0, label=\"data\"; scatter_args...)\n",
    "    smooth_step = 4\n",
    "    smooth_xs = _GP_XS[_DATA_IDXS .+ (GP_GRAIN_FACTOR ÷ 2)][(smooth_step+1):(end-smooth_step)]\n",
    "    smooth_data = [sum(data[i-smooth_step:i+smooth_step])/(2*smooth_step+1) for i in (smooth_step+1):(size(data, 1)-smooth_step)]\n",
    "    plot!(smooth_xs, smooth_data, linewidth=2, linealpha=1, ls=:dash, label=\"smooth data\"; smooth_args...)\n",
    "end;\n",
    "\n",
    "function plot_mgvi_samples(params)\n",
    "    for sample in eachcol(params.samples)\n",
    "        if any(isnan.(sample))\n",
    "            print(\"nan found in samples\", \"\\n\")\n",
    "            continue\n",
    "        end\n",
    "        plot!(_mean(Vector(sample))..., linealpha=0.5, linewidth=1, label=nothing)\n",
    "    end\n",
    "    plot!()\n",
    "end;\n",
    "\n",
    "function produce_posterior_samples(p, num_residuals)\n",
    "    batch_size = 10\n",
    "\n",
    "    if num_residuals <= 2*batch_size\n",
    "        batch_size = num_residuals ÷ 2\n",
    "    end\n",
    "\n",
    "    est_res_sampler = MGVI._create_residual_sampler(model, p;\n",
    "                                                    residual_sampler=ImplicitResidualSampler,\n",
    "                                                    jacobian_func=FwdRevADJacobianFunc,\n",
    "                                                    residual_sampler_options=(;cg_params=(;abstol=1E-2)))\n",
    "    batches = []\n",
    "    for _ in 1:(num_residuals ÷ batch_size ÷ 2)\n",
    "        batch_residual_samples = MGVI.rand(Random.GLOBAL_RNG, est_res_sampler, batch_size)\n",
    "        push!(batches, p .+ batch_residual_samples)\n",
    "        push!(batches, p .- batch_residual_samples)\n",
    "    end\n",
    "    reduce(hcat, batches)\n",
    "end\n",
    "\n",
    "function _extract_quantile(sorted_gp_realizations, p)\n",
    "    map(s -> quantile(s, p; sorted=true), eachrow(sorted_gp_realizations))\n",
    "end;\n",
    "\n",
    "function plot_posterior_bands(p, num_samples; full=false)\n",
    "    bands = [(0.997, :red), (0.955, :goldenrod1), (0.683, :green)]\n",
    "    samples = produce_posterior_samples(p, num_samples)\n",
    "    xs, first_gp = _mean(samples[1:end, 1]; full=full)\n",
    "    gp_realizations = reduce(hcat, [_mean(Vector(sample); full=full)[2] for sample in eachcol(samples[1:end, 2:end])]; init=first_gp)\n",
    "    for (i, one_x_sample) in enumerate(eachrow(gp_realizations))\n",
    "        gp_realizations[i, 1:end] .= sort(Vector(one_x_sample))\n",
    "    end\n",
    "    for (band, color) in bands\n",
    "        quant_l = _extract_quantile(gp_realizations, (1-band)/2)\n",
    "        quant_u = _extract_quantile(gp_realizations, (1+band)/2)\n",
    "        plot!(xs, quant_l; fillrange=quant_u, fillcolor=color, linealpha=0, label=band)\n",
    "    end\n",
    "    sample_median = _extract_quantile(gp_realizations, 0.5)\n",
    "    plot!(xs, sample_median; linewidth=2, linecolor=:grey25, label=\"median\")\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization and fitting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by plotting the dynamic range of the Gaussian process by\n",
    "sampling lots of possible realizations of it unconditionally on the data.\n",
    "We expect the set of lines to be populated in the region of the data."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot()\n",
    "plot_prior_samples(200)\n",
    "plot_data()\n",
    "plot!(ylim=[0, 8])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now when we see that Gaussian process potentially is able to fit the data,\n",
    "we plot the initial guess (`starting_point`) to see where do we start from.\n",
    "At this plot we show:\n",
    "* data points\n",
    "* smoothed data with moving average of 9 years\n",
    "* Poisson rate for each bin"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot()\n",
    "plot_mean(starting_point, \"starting_point\")\n",
    "plot_data()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also want to introduce the `full` plot, that shows not only the data region,\n",
    "but includes the region with the padding we added with `GP_PADDING`. We will use\n",
    "this plot to make sure that periodic boundary conditions don't interfere with\n",
    "the data."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot()\n",
    "plot_mean(starting_point, \"full gp\"; full=true)\n",
    "plot_mean(starting_point, \"starting_point\")\n",
    "plot_data()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's make a first iteration of the MGVI. We limited Optim option to 1 iteration on purpose\n",
    "to let MGVI coverge slowly, so that we'll see nice convergence curve."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "first_iteration = mgvi_kl_optimize_step(Random.GLOBAL_RNG,\n",
    "                                        model, data,\n",
    "                                        starting_point;\n",
    "                                        num_residuals=3,\n",
    "                                        jacobian_func=FwdRevADJacobianFunc,\n",
    "                                        residual_sampler=ImplicitResidualSampler,\n",
    "                                        optim_options=Optim.Options(iterations=1, show_trace=false),\n",
    "                                        residual_sampler_options=(;cg_params=(;abstol=1E-2,verbose=false)));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We again plot data and the Poisson rate. Then we again show the Gaussian process with padding.\n",
    "After one iteration Poisson rate doesn't seem to get much closer to the data."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot()\n",
    "plot_mean(first_iteration.result, \"first_iteration\")\n",
    "plot_data()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot()\n",
    "plot_data()\n",
    "plot_mean(first_iteration.result, \"full gp\"; full=true)\n",
    "plot_mean(first_iteration.result, \"first_iteration\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to visualize convergence we prepare few functions to compute\n",
    "average posterior likelihood, store it and plot it."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function compute_avg_likelihood(model, samples, data)\n",
    "    tot = 0\n",
    "    for sample in eachcol(samples)\n",
    "        tot += -MGVI.posterior_loglike(model, sample, data)\n",
    "    end\n",
    "    tot/size(samples, 2)\n",
    "end;\n",
    "\n",
    "function show_avg_likelihood(series)\n",
    "    scatter!(1:size(series, 1), series, label=\"-loglike\")\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we do 30 more iterations of the MGVI and storing average likelihood after each step.\n",
    "We feed fitted result of the previous step as an input to the next iteration."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "next_iteration = first_iteration;\n",
    "avg_likelihood_series = [];\n",
    "push!(avg_likelihood_series, compute_avg_likelihood(model, next_iteration.samples, data));\n",
    "for i in 1:30\n",
    "    tmp_iteration = mgvi_kl_optimize_step(Random.GLOBAL_RNG,\n",
    "                                          model, data,\n",
    "                                          next_iteration.result;\n",
    "                                          num_residuals=8,\n",
    "                                          jacobian_func=FwdRevADJacobianFunc,\n",
    "                                          residual_sampler=ImplicitResidualSampler,\n",
    "                                          optim_options=Optim.Options(iterations=1, show_trace=false),\n",
    "                                          residual_sampler_options=(;cg_params=(;abstol=1E-2,verbose=false)))\n",
    "    global next_iteration = tmp_iteration\n",
    "    push!(avg_likelihood_series, compute_avg_likelihood(model, next_iteration.samples, data))\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Firstly, let's have a look at the convergence plots. We see that MGVI converged after 10 iterations\n",
    "while being limited to very poor Optim performance."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(yscale=:log)\n",
    "show_avg_likelihood(avg_likelihood_series)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results itself look also good. Together with data and Poisson rate we also plot\n",
    "MGVI residuals. They are samples from the Gaussian posterior, sampled respecting the posterior\n",
    "covariance structure. Thus MGVI residual samples are deviations from the MGVI fit that represent\n",
    "how confident we are about the prediction."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(ylim=[0,8])\n",
    "plot_mgvi_samples(next_iteration)\n",
    "plot_mean(next_iteration.result, \"many_iterations\", plot_args=(color=:deepskyblue2, linewidth=3.5))\n",
    "plot_data(scatter_args=(;color=:blue2, marker_size=3.5), smooth_args=(;color=:deeppink3, linewidth=3))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To present credibility intervals we also plot credibility bands. We sample 400 residual samples\n",
    "from MGVI and then plot quantiles for each data bin. This should give us a feeling of how\n",
    "extreme are deviations of the data from the MGVI fit."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(ylim=[0,8])\n",
    "plot_posterior_bands(next_iteration.result, 400)\n",
    "plot_mean(next_iteration.result, \"many_iterations\", plot_args=(color=:deepskyblue2, linewidth=3.5))\n",
    "plot_data(scatter_args=(;color=:blue2, marker_size=3.5), smooth_args=(;color=:deeppink3, linewidth=3))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also make sure boundary conditions do not interfere with the data. Here is the Gaussian process\n",
    "plot with the paddings included:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot()\n",
    "plot_data()\n",
    "plot_mean(next_iteration.result; full=true)\n",
    "plot_mean(next_iteration.result, \"many_iterations\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Maximum A-Posteriori estimation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We build a MAP as a cross check of MGVI results. We just optimize posterior likelihood with Optim\n",
    "without any particular tuning:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "max_posterior = Optim.optimize(x -> -MGVI.posterior_loglike(model, x, data), starting_point, LBFGS(), Optim.Options(show_trace=false, g_tol=1E-10, iterations=300));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We observe that the bump in the middle is caught by MAP while it was missed by our MGVI fit:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot()\n",
    "plot_mean(Optim.minimizer(max_posterior), \"map\")\n",
    "plot_data()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also can see the difference at the left edge of the data region. While MGVI smoothed the data,\n",
    "MAP predicted a consequent peak:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot()\n",
    "plot_data()\n",
    "plot_mean(Optim.minimizer(max_posterior), \"full gp\"; full=true)\n",
    "plot_mean(Optim.minimizer(max_posterior), \"map\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.4"
  },
  "kernelspec": {
   "name": "julia-1.5",
   "display_name": "Julia 1.5.4",
   "language": "julia"
  }
 },
 "nbformat": 4
}
