<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Advanced Tutorial · MGVI</title><link rel="canonical" href="https://bat.github.io/MGVI.jl/stable/advanced_tutorial/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">MGVI</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li class="is-active"><a class="tocitem" href>Advanced Tutorial</a><ul class="internal"><li><a class="tocitem" href="#Introduction-1"><span>Introduction</span></a></li><li><a class="tocitem" href="#Prepare-the-environment-1"><span>Prepare the environment</span></a></li><li><a class="tocitem" href="#Load-data-1"><span>Load data</span></a></li><li><a class="tocitem" href="#Global-parameters-and-the-grid-1"><span>Global parameters and the grid</span></a></li><li><a class="tocitem" href="#Model-parameters-1"><span>Model parameters</span></a></li><li><a class="tocitem" href="#Model-implementation-1"><span>Model implementation</span></a></li><li><a class="tocitem" href="#Visualization-utilities-1"><span>Visualization utilities</span></a></li><li><a class="tocitem" href="#Visualization-and-fitting-1"><span>Visualization and fitting</span></a></li><li><a class="tocitem" href="#Maximum-A-Posteriori-estimation-1"><span>Maximum A-Posteriori estimation</span></a></li></ul></li><li><a class="tocitem" href="../api/">API</a></li><li><a class="tocitem" href="../LICENSE/">LICENSE</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Advanced Tutorial</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Advanced Tutorial</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/bat/MGVI.jl/blob/master/docs/build/src/advanced_tutorial_lit.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Advanced-Tutorial-1"><a class="docs-heading-anchor" href="#Advanced-Tutorial-1">Advanced Tutorial</a><a class="docs-heading-anchor-permalink" href="#Advanced-Tutorial-1" title="Permalink"></a></h1><p>Notebook <a href="../advanced_tutorial.ipynb">download</a> <a href="https://nbviewer.jupyter.org/github/bat/MGVI.jl/blob/gh-pages/previews/PR23/advanced_tutorial.ipynb">nbviewer</a> <a href="../advanced_tutorial.jl">source</a></p><h2 id="Introduction-1"><a class="docs-heading-anchor" href="#Introduction-1">Introduction</a><a class="docs-heading-anchor-permalink" href="#Introduction-1" title="Permalink"></a></h2><p>In this tutorial, we will fit the <a href="../coal_mining_data.tsv">coal mining disaster dataset</a> with a Gaussian process modulated Poisson process.</p><h2 id="Prepare-the-environment-1"><a class="docs-heading-anchor" href="#Prepare-the-environment-1">Prepare the environment</a><a class="docs-heading-anchor-permalink" href="#Prepare-the-environment-1" title="Permalink"></a></h2><p>We start by importing:</p><ul><li>MGVI that we will use for posterior fit</li><li>Distributions and FFTW to define the statistical model</li><li>Optim to pass Optim.Options to MGVI and to find Maximum A-Posteriori fit that we will use for comparison</li><li>StatsBase for histogram preparation from the data and also for error bands visualization</li><li>Plots.jl for visualization</li></ul><pre><code class="language-julia">using MGVI

using Distributions
using DelimitedFiles
using Random
using Optim
using StatsBase

using Plots

using FFTW

import ForwardDiff</code></pre><pre><code class="language-julia">Random.seed!(84612);
nothing #hide</code></pre><h2 id="Load-data-1"><a class="docs-heading-anchor" href="#Load-data-1">Load data</a><a class="docs-heading-anchor-permalink" href="#Load-data-1" title="Permalink"></a></h2><p>Dataset is attached to the repository and contains intervals in days between disasters happend at British coal mines between March 1851 and March 1962. We split the entire time range into intervals of 365 days, then the number of events in each interval costitutes the measurement we are going to model.</p><pre><code class="language-julia">function read_coal_mining_data(filepath, binsize)
    init_year = empty
    data = empty
    open(filepath) do io
        raw = readline(io)
        while ! occursin(&quot;init_date&quot;, raw)
            raw = readline(io)
        end

        init_year = parse(Float64, split(split(strip(raw[2:end]), &quot;\t&quot;)[2], &quot;-&quot;)[1])
        data = readdlm(io, &#39;\t&#39;, Int, &#39;\n&#39;, comments=true)[:]
    end
    dates_fract_years = init_year .+ cumsum(data)/365
    left_edge = dates_fract_years[1]
    num_bins = ((dates_fract_years[end] - left_edge) ÷ binsize)
    right_edge = left_edge + binsize*num_bins
    fit(Histogram, dates_fract_years, left_edge:binsize:right_edge).weights
end

coal_mine_disaster_data = read_coal_mining_data(joinpath(&quot;/home/runner/work/MGVI.jl/MGVI.jl/docs/build/src&quot;, &quot;coal_mining_data.tsv&quot;), 1);
nothing #hide</code></pre><h2 id="Global-parameters-and-the-grid-1"><a class="docs-heading-anchor" href="#Global-parameters-and-the-grid-1">Global parameters and the grid</a><a class="docs-heading-anchor-permalink" href="#Global-parameters-and-the-grid-1" title="Permalink"></a></h2><p>Now we define several model properties:</p><ul><li><code>DATA_DIM</code> is just the size of the dataset</li><li><code>DATA_XLIM</code> specifies the time range of the data</li><li><code>GP_GRAIN_FACTOR</code> determines the numbers of finer bins into which the data bin is split.  This is useful when there are several datasets defined on different grids.</li><li><code>GP_PADDING</code> adds empty paddings to the dataset. We use Fourier transform to sample from the Gaussian process  with a finite correlation length. <code>GP_PADDING</code> helps us to ensure that periodic boundary conditions  imposed by Fourier transform won&#39;t affect the data region.</li></ul><pre><code class="language-julia">DATA_DIM = size(coal_mine_disaster_data, 1);

data = coal_mine_disaster_data;

DATA_XLIM = [1851., 1962.];

GP_GRAIN_FACTOR = 3;
GP_PADDING = 80;
nothing #hide</code></pre><pre><code class="language-julia">function produce_bins()
    data_binsize = (DATA_XLIM[2] - DATA_XLIM[1])/DATA_DIM
    gp_binsize = data_binsize/GP_GRAIN_FACTOR
    gp_dim = Integer(((DATA_XLIM[2] - DATA_XLIM[1]) + 2*GP_PADDING) ÷ gp_binsize)
    gp_left_bin_offset = gp_right_bin_offset = (gp_dim - DATA_DIM) ÷ 2
    if (2*gp_left_bin_offset + DATA_DIM*GP_GRAIN_FACTOR) % 2 == 1
        gp_left_bin_offset += 1
    end
    gp_left_xlim = DATA_XLIM[1] - gp_left_bin_offset*gp_binsize
    gp_right_xlim = DATA_XLIM[2] + gp_right_bin_offset*gp_binsize
    gp_left_xs = collect(gp_left_xlim + gp_binsize/2:gp_binsize:DATA_XLIM[1])
    gp_right_xs = collect(DATA_XLIM[2] + gp_binsize/2:gp_binsize:gp_right_xlim)
    gp_data_xs = collect(DATA_XLIM[1] + gp_binsize/2:gp_binsize:DATA_XLIM[2])
    gp_xs = [gp_left_xs; gp_data_xs; gp_right_xs]
    data_idxs = collect(gp_left_bin_offset+1:GP_GRAIN_FACTOR:gp_left_bin_offset+DATA_DIM*GP_GRAIN_FACTOR)
    gp_xs, gp_binsize, data_idxs
end;
nothing #hide</code></pre><p>Based on the defined model properties, we generate the grid. GP grid is the fine-grained grid with offsets added to the data range.</p><ul><li><code>_GP_XS</code> represent bin centers of such a fine-grained grid</li><li><code>_GP_BINSIZE</code> is the width of the bin (that is 1/<code>GP_GRAIN_FACTOR</code> of data bin size)</li><li><code>_DATA_IDXS</code> - integer indices of the left edges of the data bins</li></ul><pre><code class="language-julia">_GP_XS, _GP_BINSIZE, _DATA_IDXS = produce_bins();
_GP_DIM = length(_GP_XS);
nothing #hide</code></pre><h2 id="Model-parameters-1"><a class="docs-heading-anchor" href="#Model-parameters-1">Model parameters</a><a class="docs-heading-anchor-permalink" href="#Model-parameters-1" title="Permalink"></a></h2><p>The Gaussian process in this tutorial is modeled in the Fourier space with zero mean and two hyperparameters defining properties of its kernel. To sample from this Gaussian process, we also need a parameter per bin that will represent the particular realization of the GP in the bin.</p><pre><code class="language-julia">function assemble_paridx(;kwargs...)
    pos = 0
    res = []
    for (k, v) in kwargs
        new_start, new_stop = v.start+pos, v.stop+pos
        push!(res, (k, (v.start+pos):(v.stop+pos)))
        pos = new_stop
    end
    (;res...)
end;
nothing #hide</code></pre><p>MGVI is an iterative procedure, so we will need to introduce an initial guess for the state of the model. We create one vector of the size of the count of all parameters <code>starting_point</code> and a NamedTuple <code>PARDIX</code> that assigns names to the sub-regions in the vector of parameters. In the correct case:</p><ul><li><code>gp_hyper</code> two hyperparameters of the Gaussian process stored in the first two cells of the parameter vector</li><li><code>gp_latent</code> <code>_GP_DIM</code> parameters used to define the particular realization of the gaussian process,  stored at indices between <code>3</code> to <code>2 + _GP_DIM</code>.</li></ul><p>Function <code>assemble_paridx</code> is responsible for constructing such a NamedTuple from the parameter specification.</p><pre><code class="language-julia">PARIDX = assemble_paridx(gp_hyper=1:2, gp_latent=1:_GP_DIM);

starting_point = randn(last(PARIDX).stop);
nothing #hide</code></pre><h2 id="Model-implementation-1"><a class="docs-heading-anchor" href="#Model-implementation-1">Model implementation</a><a class="docs-heading-anchor-permalink" href="#Model-implementation-1" title="Permalink"></a></h2><pre><code class="language-julia">k = collect(0:(_GP_DIM)÷2 -1);
nothing #hide</code></pre><p>Gaussian process covariance in the Fourier space is represented with a diagonal matrix. Values on the diagonal follow squared exponential function with parameters depending on priors. A kernel that is diagonal and mirrored around the center represents periodic and translation invariant function in the coordinate space. This property restricts covariance to have a finite correlation length in the coordinate space.</p><p>MGVI assumes that all priors are distributed as standard normals <code>N(0, 1)</code>; thus, to modify the shapes of the priors, we explicitly rescale them at the model implementation phase.</p><p>We also exponentiate each prior before using it for tuning squared exponential shape. In this way, we impose only positive values of the hyperparameters of the kernel.</p><p>Actually, for the sake of numeric stability we model already square root of the covariance. This can be traced by missing <code>sqrt</code> in the next level, where we sample from the Gaussian process.</p><pre><code class="language-julia">function kernel_model(p)
    kernel_A_c, kernel_l_c = p[PARIDX.gp_hyper]
    kernel_A = 60*exp(kernel_A_c*0.9)*GP_GRAIN_FACTOR
    kernel_l = 0.025*exp(kernel_l_c/15)/(GP_GRAIN_FACTOR^0.3)
    positive_modes = kernel_A .* sqrt(2 * π * kernel_l) .* exp.( -π^2 .* k.^2 .* kernel_l^2)
    negative_modes = positive_modes[end:-1:1]
    [positive_modes; negative_modes]
end;
nothing #hide</code></pre><p>As a Fourier transform we use Discrete Hartley Transform that ensures that Fourier coefficients of the real valued function are real valued.</p><pre><code class="language-julia">ht = FFTW.plan_r2r(zeros(_GP_DIM), FFTW.DHT);
nothing #hide</code></pre><p>Before we proceed, let&#39;s have a brief look at the kernel&#39;s shape. Below we plot the kernel in the coordinate space <code>K(r) = K(x2 - x1)</code> as a function of time in years between two points. The further we go along <code>x</code>-axis the larger is the time interval and the smaller is the covariance between two points in time.</p><pre><code class="language-julia">function plot_kernel_model(p, width; plot_args=(;))
    xs = collect(1:Int(floor(width/_GP_BINSIZE)))
    plot!(xs .* _GP_BINSIZE, (ht * kernel_model(p))[xs] ./ _GP_DIM, label=nothing, linewidth=2.5; plot_args...)
end

plot()
plot_kernel_model(starting_point, 20)</code></pre><p><img src="../280718108.png" alt/></p><p>To make it even more visual, we also plot the structure of the covariance matrix as a heatmap. We see that the finite correlation length shows up as a band around the diagonal. We also see small artifacts in the antidiagonal corners. They come from the assumption that the kernel is periodic.</p><pre><code class="language-julia">function plot_kernel_matrix(p)
    xkernel = ht * kernel_model(p) ./ _GP_DIM
    res = reduce(hcat, [circshift(xkernel, i) for i in _GP_DIM-1:-1:0])&#39;
    heatmap!(res)
end

plot()
plot_kernel_matrix(starting_point)</code></pre><p><img src="../2862521961.png" alt/></p><p>After we defined the square root of the kernel function (<code>kernel_model</code>), we just follow the regular procedure of sampling from the normal distribution. Since the covariance matrix in the Fourier space is diagonal, Gaussian variables in each bin are independent of each other. Thus, sampling ends up rescaling the <code>gp_latent</code> part of the prior vector responsible for the Gaussian process state.</p><p>After we produced a sample of Gaussian random values following the kernel model, we apply Fourier transform to return back to the coordinate space.</p><pre><code class="language-julia">function gp_sample(p)
    flat_gp = kernel_model(p) .* p[PARIDX.gp_latent]
    (ht * flat_gp) ./ _GP_DIM
end;
nothing #hide</code></pre><p>Together with the implementation of <code>gp_sample</code> we also need it&#39;s version for the <code>Dual</code>s. It is a little patch that makes application of the Hartley transform differentiatiable.</p><pre><code class="language-julia">function gp_sample(dp::Vector{ForwardDiff.Dual{T, V, N}}) where {T,V,N}
    flat_gp_duals = kernel_model(dp) .* dp[PARIDX.gp_latent]
    val_res = ht*ForwardDiff.value.(flat_gp_duals) ./ _GP_DIM
    psize = size(ForwardDiff.partials(flat_gp_duals[1]), 1)
    ps = x -&gt; ForwardDiff.partials.(flat_gp_duals, x)
    val_ps = map((x -&gt; ht*ps(x) ./ _GP_DIM), 1:psize)
    ForwardDiff.Dual{T}.(val_res, val_ps...)
end;
nothing #hide</code></pre><p>Gaussian process realization is meant to serve as a Poisson rate of the Poisson process. Since the Gaussian process is not restricted to positive values, we exponentiate its values to forcefully make the function positive.</p><pre><code class="language-julia">function poisson_gp_link(fs)
    exp.(fs)
end;
nothing #hide</code></pre><p>Now when we have a function representing the Poisson rate density, we have to integrate it over each data bin to define the Poisson rate in these bins. Function <code>agg_lambdas</code> does precisely that. When <code>GP_GRAIN_FACTOR = 1</code>, this function just multiplies the value of the Gaussian process in the bin by the <code>_GP_BINSIZE</code>. When we have more GP bins per data bin (<code>GP_GRAIN_FACTOR &gt; 1</code>), then we apply rectangular quadrature to integrate over the bin.</p><pre><code class="language-julia">function _forward_agg(data, idxs, steps_forward)
    [sum(data[i:i+steps_forward-1]) for i in idxs]
end;

function agg_lambdas(lambdas)
    gps = _forward_agg(lambdas, _DATA_IDXS, GP_GRAIN_FACTOR) .* _GP_BINSIZE
    xs = _GP_XS[_DATA_IDXS .+ (GP_GRAIN_FACTOR ÷ 2)]
    xs, gps
end;
nothing #hide</code></pre><p>Finally, we arrive to the model definition assembled from the building blocks we defined above:</p><ul><li><code>gp_sample</code> sample from the Gaussian process with defined <code>kernel_model</code> covariance</li><li><code>poisson_gp_link</code> ensures Gaussian process is positive</li><li><code>agg_lambdas</code> integrates Gaussian process over each data bin to turn it into Poisson rate for each bin</li><li><code>model</code> maps parameters into the product of the Poisson distributions counting events in each bin.</li></ul><pre><code class="language-julia">function model(params)
    fs = gp_sample(params)
    fine_lambdas = poisson_gp_link(fs)
    _, lambdas = agg_lambdas(fine_lambdas)
    Product(Poisson.(lambdas))
end;
nothing #hide</code></pre><h2 id="Visualization-utilities-1"><a class="docs-heading-anchor" href="#Visualization-utilities-1">Visualization utilities</a><a class="docs-heading-anchor-permalink" href="#Visualization-utilities-1" title="Permalink"></a></h2><pre><code class="language-julia">function agg_full_lambdas(lambdas)
    left_idxs = 1:GP_GRAIN_FACTOR:(_DATA_IDXS[1]-GP_GRAIN_FACTOR)
    left_gp = _forward_agg(lambdas, left_idxs, GP_GRAIN_FACTOR) .* _GP_BINSIZE
    left_xs = _GP_XS[left_idxs .+ (GP_GRAIN_FACTOR ÷ 2)]
    right_idxs = (_DATA_IDXS[end]+1):GP_GRAIN_FACTOR:(size(lambdas, 1) - GP_GRAIN_FACTOR)
    right_gp = _forward_agg(lambdas, right_idxs, GP_GRAIN_FACTOR) .* _GP_BINSIZE
    right_xs = _GP_XS[right_idxs .+ (GP_GRAIN_FACTOR ÷ 2)]
    middle_xs, middle_gp = agg_lambdas(lambdas)
    full_xs = [left_xs; middle_xs; right_xs]
    full_gp = [left_gp; middle_gp; right_gp]
    full_xs, full_gp
end;

function _mean(p; full=false)
    agg_func = if (!full) agg_lambdas else agg_full_lambdas end
    xs, gps = agg_func(poisson_gp_link(gp_sample(p)))
    xs, gps
end;

function plot_mean(p, label=&quot;mean&quot;; plot_args=(;), full=false)
    plot!(_mean(p; full=full)..., label=label, linewidth=2; plot_args...)
end;

function plot_prior_samples(num_samples)
    for _ in 1:num_samples
        p = randn(last(PARIDX).stop)
        plot_mean(p, nothing)
    end
end;

function plot_kernel_prior_samples(num_samples, width)
    for _ in 1:num_samples
        p = randn(last(PARIDX).stop)
        plot_kernel_model(p, width)
    end
    plot!()
end;

function plot_data(; scatter_args=(;), smooth_args=(;))
    scatter!(_GP_XS[_DATA_IDXS .+ (GP_GRAIN_FACTOR ÷ 2)], data, la=0, markersize=2., markerstrokewidth=0, label=&quot;data&quot;; scatter_args...)
    smooth_step = 4
    smooth_xs = _GP_XS[_DATA_IDXS .+ (GP_GRAIN_FACTOR ÷ 2)][(smooth_step+1):(end-smooth_step)]
    smooth_data = [sum(data[i-smooth_step:i+smooth_step])/(2*smooth_step+1) for i in (smooth_step+1):(size(data, 1)-smooth_step)]
    plot!(smooth_xs, smooth_data, linewidth=2, linealpha=1, ls=:dash, label=&quot;smooth data&quot;; smooth_args...)
end;

function plot_mgvi_samples(params)
    for sample in eachcol(params.samples)
        if any(isnan.(sample))
            print(&quot;nan found in samples&quot;, &quot;\n&quot;)
            continue
        end
        plot!(_mean(Vector(sample))..., linealpha=0.5, linewidth=1, label=nothing)
    end
    plot!()
end;

function plot_kernel_mgvi_samples(params, width)
    for sample in eachcol(params.samples)
        if any(isnan.(sample))
            print(&quot;nan found in samples&quot;, &quot;\n&quot;)
            continue
        end
        plot_kernel_model(sample, width; plot_args=(linealpha=0.5, linewidth=1, label=nothing))
    end
    plot!()
end;

function produce_posterior_samples(p, num_residuals)
    batch_size = 10

    if num_residuals &lt;= 2*batch_size
        batch_size = num_residuals ÷ 2
    end

    est_res_sampler = MGVI._create_residual_sampler(model, p;
                                                    residual_sampler=ImplicitResidualSampler,
                                                    jacobian_func=FwdRevADJacobianFunc,
                                                    residual_sampler_options=(;cg_params=(;abstol=1E-2)))
    batches = []
    for _ in 1:(num_residuals ÷ batch_size ÷ 2)
        batch_residual_samples = MGVI.rand(Random.GLOBAL_RNG, est_res_sampler, batch_size)
        push!(batches, p .+ batch_residual_samples)
        push!(batches, p .- batch_residual_samples)
    end
    reduce(hcat, batches)
end

function _extract_quantile(sorted_gp_realizations, p)
    map(s -&gt; quantile(s, p; sorted=true), eachrow(sorted_gp_realizations))
end;

function plot_posterior_bands(p, num_samples; full=false)
    bands = [(0.997, :red), (0.955, :goldenrod1), (0.683, :green)]
    samples = produce_posterior_samples(p, num_samples)
    xs, first_gp = _mean(samples[1:end, 1]; full=full)
    gp_realizations = reduce(hcat, [_mean(Vector(sample); full=full)[2] for sample in eachcol(samples[1:end, 2:end])]; init=first_gp)
    for (i, one_x_sample) in enumerate(eachrow(gp_realizations))
        gp_realizations[i, 1:end] .= sort(Vector(one_x_sample))
    end
    for (band, color) in bands
        quant_l = _extract_quantile(gp_realizations, (1-band)/2)
        quant_u = _extract_quantile(gp_realizations, (1+band)/2)
        plot!(xs, quant_l; fillrange=quant_u, fillcolor=color, linealpha=0, label=band)
    end
    sample_median = _extract_quantile(gp_realizations, 0.5)
    plot!(xs, sample_median; linewidth=2, linecolor=:grey25, label=&quot;median&quot;)
end;
nothing #hide</code></pre><h2 id="Visualization-and-fitting-1"><a class="docs-heading-anchor" href="#Visualization-and-fitting-1">Visualization and fitting</a><a class="docs-heading-anchor-permalink" href="#Visualization-and-fitting-1" title="Permalink"></a></h2><p>We start by plotting the Gaussian process&#39;s dynamic range by sampling many possible realizations of it unconditionally on the data. We expect the set of lines to populate the regions where there are data.</p><pre><code class="language-julia">plot()
plot_prior_samples(200)
plot_data()
plot!(ylim=[0, 8])</code></pre><p><img src="../1856666012.png" alt/></p><p>We also plot prior samples for the kernel in the coordinate space. The plot below shows that the kernel is flexible in the amplitude while the correlation length is quite strongly predefined:</p><pre><code class="language-julia">plot()
plot_kernel_prior_samples(200, 20)</code></pre><p><img src="../1530237926.png" alt/></p><p>Now we see that the Gaussian process potentially is able to fit the data, we plot the initial guess (<code>starting_point</code>) to see where we start from. This plot shows:</p><ul><li>data points</li><li>smoothed data with a moving average of 9 years</li><li>Poisson rate for each bin</li></ul><pre><code class="language-julia">plot()
plot_mean(starting_point, &quot;starting_point&quot;)
plot_data()</code></pre><p><img src="../1483865695.png" alt/></p><p>We also want to introduce the <code>full</code> plot that shows not only the data region but includes the region with the padding we added with <code>GP_PADDING</code>. We will use this plot to make sure that periodic boundary conditions don&#39;t interfere with the data.</p><pre><code class="language-julia">plot()
plot_mean(starting_point, &quot;full gp&quot;; full=true)
plot_mean(starting_point, &quot;starting_point&quot;)
plot_data()</code></pre><p><img src="../1372009517.png" alt/></p><p>Let&#39;s make a first iteration of the MGVI. We limited Optim option to 1 iteration on purpose to let MGVI coverge slowly, so that we&#39;ll see nice convergence curve.</p><pre><code class="language-julia">first_iteration = mgvi_kl_optimize_step(Random.GLOBAL_RNG,
                                        model, data,
                                        starting_point;
                                        num_residuals=3,
                                        jacobian_func=FwdRevADJacobianFunc,
                                        residual_sampler=ImplicitResidualSampler,
                                        optim_options=Optim.Options(iterations=1, show_trace=false),
                                        residual_sampler_options=(;cg_params=(;abstol=1E-2,verbose=false)));
nothing #hide</code></pre><p>We again plot data and the Poisson rate. Then we again show the Gaussian process with padding. After one iteration the Poisson rate doesn&#39;t seem to get much closer to the data.</p><pre><code class="language-julia">plot()
plot_mean(first_iteration.result, &quot;first_iteration&quot;)
plot_data()</code></pre><p><img src="../4232071849.png" alt/></p><pre><code class="language-julia">plot()
plot_data()
plot_mean(first_iteration.result, &quot;full gp&quot;; full=true)
plot_mean(first_iteration.result, &quot;first_iteration&quot;)</code></pre><p><img src="../679683408.png" alt/></p><p>We also would like to have a look at the kernel. Below we plot it together with the MGVI samples that represent the possible variation of the kernel shape around the mean:</p><pre><code class="language-julia">plot()
plot_kernel_model(first_iteration.result, 20; plot_args=(;label=&quot;kernel model&quot;))
plot_kernel_mgvi_samples(first_iteration, 20)</code></pre><p><img src="../817780611.png" alt/></p><p>In order to visualize convergence we prepare few functions to compute average posterior likelihood, store it and plot it.</p><pre><code class="language-julia">function compute_avg_likelihood(model, samples, data)
    tot = 0
    for sample in eachcol(samples)
        tot += -MGVI.posterior_loglike(model, sample, data)
    end
    tot/size(samples, 2)
end;

function show_avg_likelihood(series)
    scatter!(1:size(series, 1), series, label=&quot;-loglike&quot;)
end;
nothing #hide</code></pre><p>Now we do 30 more iterations of the MGVI and store the average likelihood after each step. We feed the fitted result of the previous step as an input to the next iteration.</p><pre><code class="language-julia">next_iteration = first_iteration;
avg_likelihood_series = [];
push!(avg_likelihood_series, compute_avg_likelihood(model, next_iteration.samples, data));
for i in 1:30
    tmp_iteration = mgvi_kl_optimize_step(Random.GLOBAL_RNG,
                                          model, data,
                                          next_iteration.result;
                                          num_residuals=8,
                                          jacobian_func=FwdRevADJacobianFunc,
                                          residual_sampler=ImplicitResidualSampler,
                                          optim_options=Optim.Options(iterations=1, show_trace=false),
                                          residual_sampler_options=(;cg_params=(;abstol=1E-2,verbose=false)))
    global next_iteration = tmp_iteration
    push!(avg_likelihood_series, compute_avg_likelihood(model, next_iteration.samples, data))
end;
nothing #hide</code></pre><p>Firstly, let&#39;s have a look at the convergence plots. We see that MGVI converged after 10 iterations while being limited to very poor Optim performance.</p><pre><code class="language-julia">plot(yscale=:log)
show_avg_likelihood(avg_likelihood_series)</code></pre><p><img src="../2634630672.png" alt/></p><p>Below we plot the result of the fit. Together with data and Poisson rate we also plot MGVI residuals. They are samples from the Gaussian posterior, sampled respecting the posterior covariance structure. Thus MGVI residual samples are deviations from the MGVI fit that represent how confident we are about the prediction.</p><pre><code class="language-julia">plot(ylim=[0,8])
plot_mgvi_samples(next_iteration)
plot_mean(next_iteration.result, &quot;many_iterations&quot;, plot_args=(color=:deepskyblue2, linewidth=3.5))
plot_data(scatter_args=(;color=:blue2, marker_size=3.5), smooth_args=(;color=:deeppink3, linewidth=3))</code></pre><p><img src="../468016744.png" alt/></p><p>To present credibility intervals we also plot credibility bands. We sample 400 residual samples from MGVI and then plot quantiles for each data bin. This should give us a feeling of how the MGVI fit is compatible with the data.</p><pre><code class="language-julia">plot(ylim=[0,8])
plot_posterior_bands(next_iteration.result, 400)
plot_mean(next_iteration.result, &quot;many_iterations&quot;, plot_args=(color=:deepskyblue2, linewidth=3.5))
plot_data(scatter_args=(;color=:blue2, marker_size=3.5), smooth_args=(;color=:deeppink3, linewidth=3))</code></pre><p><img src="../1080416875.png" alt/></p><p>We also make sure boundary conditions do not interfere with the data. Here is the Gaussian process plot with the paddings included:</p><pre><code class="language-julia">plot()
plot_data()
plot_mean(next_iteration.result; full=true)
plot_mean(next_iteration.result, &quot;many_iterations&quot;)</code></pre><p><img src="../2208713182.png" alt/></p><p>Let&#39;s have a look at the kernel again. We expect the variation of samples to become narrower:</p><pre><code class="language-julia">plot()
plot_kernel_model(next_iteration.result, 20; plot_args=(;label=&quot;kernel model&quot;))
plot_kernel_mgvi_samples(next_iteration, 20)</code></pre><p><img src="../1316004145.png" alt/></p><h2 id="Maximum-A-Posteriori-estimation-1"><a class="docs-heading-anchor" href="#Maximum-A-Posteriori-estimation-1">Maximum A-Posteriori estimation</a><a class="docs-heading-anchor-permalink" href="#Maximum-A-Posteriori-estimation-1" title="Permalink"></a></h2><p>We build a MAP as a cross check of MGVI results. We just optimize posterior likelihood with Optim without any particular tuning:</p><pre><code class="language-julia">max_posterior = Optim.optimize(x -&gt; -MGVI.posterior_loglike(model, x, data), starting_point, LBFGS(), Optim.Options(show_trace=false, g_tol=1E-10, iterations=300));
nothing #hide</code></pre><p>We observe that the bump in the middle (around 1910) is caught by MAP while it is less pronounced in the MGVI fit. MAP also has finer structure around 1875 and 1835.</p><pre><code class="language-julia">plot()
plot_mean(Optim.minimizer(max_posterior), &quot;map&quot;)
plot_mean(next_iteration.result, &quot;mgvi mean&quot;)
plot_data()</code></pre><p><img src="../3261511658.png" alt/></p><p>We also can see the difference at the left edge of the data region. While MGVI smoothed the data, MAP predicted a consequent peak:</p><pre><code class="language-julia">plot()
plot_data()
plot_mean(Optim.minimizer(max_posterior), &quot;full gp&quot;; full=true)
plot_mean(next_iteration.result, &quot;mgvi full gp&quot;; full=true)</code></pre><p><img src="../1232900436.png" alt/></p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../tutorial/">« Tutorial</a><a class="docs-footer-nextpage" href="../api/">API »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 23 March 2021 20:12">Tuesday 23 March 2021</span>. Using Julia version 1.5.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
