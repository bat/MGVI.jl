{
 "cells": [
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MGVI"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Distributions\n",
    "using Random\n",
    "using ValueShapes\n",
    "using LinearAlgebra\n",
    "using Optim"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to fit a 3-degree polynomial using two data sets (`a` and `b`). MGVI requires a model\n",
    "expressed as a function of the model parameters and returning an instance of the Distribution.\n",
    "In this example, since we have two sets of independent measurements, we express them as\n",
    "ValueShapes.NamedTupleDist.\n",
    "\n",
    "\n",
    "We assume errors are normally distributed with unknown covariance, which has to be learned as well."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "const _x1_grid = [Float64(i)/10 for i in 1:25]\n",
    "const _x2_grid = [Float64(i)/10 + 0.1 for i in 1:15]\n",
    "const _common_grid = sort(vcat(_x1_grid, _x2_grid))\n",
    "\n",
    "function _mean(x_grid, p)\n",
    "    p[1]*10 .+ p[2]*40 .* x_grid .+ p[3]*600 .* x_grid.^2 .+ p[4]*80 .* x_grid.^3\n",
    "end\n",
    "\n",
    "function model(p)\n",
    "    dist1 = Product(Normal.(_mean(_x1_grid, p), p[5]^2*60))\n",
    "    dist2 = Product(Normal.(_mean(_x2_grid, p), p[5]^2*60))\n",
    "    NamedTupleDist(a=dist1,\n",
    "                   b=dist2)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we define the ground truth of the parameters, as well as an initial guess."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "const true_params =  [\n",
    " -0.3\n",
    " -1.5\n",
    " 0.2\n",
    " -0.5\n",
    " 0.3]\n",
    "\n",
    "const starting_point = [\n",
    "  0.2\n",
    "  0.5\n",
    "  -0.1\n",
    "  0.3\n",
    " -0.6\n",
    "];"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function pprintln(obj)\n",
    "    show(stdout, \"text/plain\", obj)\n",
    "    println()\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Plots\n",
    "gr(size=(400, 300), dpi=700, fmt=:png)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "rng = MersenneTwister(157);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We draw data directly from the model, using the true parameter values:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "data = rand(rng, model(true_params), 1)[1];"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function _mean(x::Vector)\n",
    "    _mean(_common_grid, x)\n",
    "end\n",
    "\n",
    "init_plots =() -> let\n",
    "    truth = _mean(true_params)\n",
    "    plot!(_common_grid, truth, markercolor=:blue, linecolor=:blue, label=\"truth\")\n",
    "    scatter!(_common_grid, _mean(starting_point), markercolor=:orange, markerstrokewidth=0, markersize=3, label=\"init\")\n",
    "    scatter!(vcat(_x1_grid, _x2_grid), reduce(vcat, data), markercolor=:black, markerstrokewidth=0, markersize=3, label=\"data\")\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we start the optimization, let's have an initial look at the data.\n",
    "It is also interesting to see how our starting guess performs."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "p = plot()\n",
    "init_plots()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are ready to run one iteration of the MGVI.\n",
    "The output contains an updated parameter estimate (`first_iteration.result`),\n",
    "which we can compare to the true parameters."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "first_iteration = mgvi_kl_optimize_step(rng,\n",
    "                                        model, data,\n",
    "                                        starting_point;\n",
    "                                        jacobian_func=FwdRevADJacobianFunc,\n",
    "                                        residual_sampler=ImplicitResidualSampler,\n",
    "                                        optim_options=Optim.Options(iterations=10, show_trace=true),\n",
    "                                        residual_sampler_options=(;cg_params=(;maxiter=10)))\n",
    "pprintln(hcat(first_iteration.result, true_params))\n",
    "p"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot_iteration = (params, label) -> let\n",
    "    #error_mat = mgvi_kl_errors(full_model, params)\n",
    "    #display(error_mat)\n",
    "    #errors = sqrt.(error_mat[diagind(error_mat)])\n",
    "    #yerr = abs.(line(common_grid, params+errors) - line(common_grid, params-errors))\n",
    "    #scatter!(common_grid, line(common_grid, params), markercolor=:green, label=label, yerr=yerr)\n",
    "    for sample in eachcol(params.samples)\n",
    "        scatter!(_common_grid, _mean(Vector(sample)), markercolor=:gray, markeralpha=0.3, markersize=2, label=nothing)\n",
    "    end\n",
    "    scatter!(_common_grid, _mean(params.result), markercolor=:green, label=label)\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's also plot the curve corresponding to the new parameters after the first iteration:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "p = plot()\n",
    "init_plots()\n",
    "plot_iteration(first_iteration, \"first\")\n",
    "p"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot_iteration_light = (params, counter) -> let\n",
    "    scatter!(_common_grid, _mean(params.result), markercolor=:green, markersize=3, markeralpha=2*atan(counter/18)/Ï€, label=nothing)\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the plot above we see that one iteration is not enough. Let's do 5 more steps and plot the evolution of estimates."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "init_plots()\n",
    "plt = scatter()\n",
    "next_iteration = first_iteration\n",
    "for i in 1:5\n",
    "    pprintln(minimum(next_iteration.optimized))\n",
    "    pprintln(hcat(next_iteration.result, true_params))\n",
    "    global next_iteration = mgvi_kl_optimize_step(rng,\n",
    "                                                  model, data,\n",
    "                                                  next_iteration.result;\n",
    "                                                  jacobian_func=FwdRevADJacobianFunc,\n",
    "                                                  residual_sampler=ImplicitResidualSampler,\n",
    "                                                  optim_options=Optim.Options(iterations=10, show_trace=true),\n",
    "                                                  residual_sampler_options=(;cg_params=(;maxiter=10)))\n",
    "    plot_iteration_light(next_iteration, i)\n",
    "end\n",
    "pprintln(minimum(next_iteration.optimized))\n",
    "pprintln(hcat(next_iteration.result, true_params))\n",
    "plt"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, let's plot the last estimate and compare it to the truth. Also, notice, that gray dots represent samples from\n",
    "the approximation."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "p = plot()\n",
    "init_plots()\n",
    "plot_iteration(next_iteration, \"last\")\n",
    "p"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
